
# Go-Exploit: Targeted Search Control in AlphaZero for Effective Policy Improvement

## Motivation:

AlphaZero is a model-based reinforcement learning (RL) algorithm that has achieved impressive results in two-player, zero-sum games, reaching superhuman play in chess, shogi, and Go. AlphaZero simulates self-play matches with a perfect model of its environment (the rules of the game) to train a neural network that learns a value function and action selection priors over states. Each turn, the value function and priors guide a lookahead search that returns an improved policy. AlphaZero trains its neural network on the self-play matches produced under the improved policies, enabling it to improve its play via policy iteration.

Despite its success, AlphaZero's training suffers from sample inefficiency. In 19x19 Go, AlphaZero requires hundreds of millions of training samples to attain superhuman play. AlphaZero's sample efficiency depends upon the distribution of states visited and trained upon. Although AlphaZero has a perfect model of its environment, it cannot feasibly visit and learn the optimal value for each state. Instead, AlphaZero trains upon the states that it visits on-policy in simulated self-play matches beginning from the initial state of the game. As in other RL algorithms, AlphaZero takes exploratory actions during its self-play matches so that it can train upon a variety of states, enabling it to make more informed action selections in the future. AlphaZero employs simplistic exploration mechanisms during self-play training: randomly perturbing the learned priors guiding search and stochastically selecting actions near the start of self-play matches. As a result, AlphaZero's training procedure exhibits the following limitations:

1. Since AlphaZero begins its self-play matches from the initial state of a game, it often transitions into a terminal state before reaching and exploring states deeper in the game tree. In addition, AlphaZero only samples actions over the first few moves of a self-play match, further limiting exploration deeper in the game tree.
2. AlphaZero's exploration mechanisms cause it to train under weaker, exploratory policies, slowing policy iteration.
3. AlphaZero only produces a single, noisy value target from a full self-play match, slowing value training.

We hypothesized that AlphaZero could address these limitations, and learn with greater sample efficiency, with a more effective search control strategy. Sutton and Barto define search control as "the process that selects the starting states and actions for the simulated experiences generated by the model". In AlphaZero, this would amount to strategically choosing the starting state of its simulated trajectories. We propose one such strategy that adheres to four guiding principles. The algorithm should:

- Continually visit new states throughout the state space to learn their values and a good policy.
- Keep track of states of interest and have the ability to reliably revisit them for further exploration.
- Limit exploration’s bias in the learning targets.
- Produce more independent value targets to train upon.

We introduce Go-Exploit, a novel search control strategy for AlphaZero. Go-Exploit takes inspiration from Go-Explore and Exploring Restart Distributions, which begin simulated episodes from previously visited states sampled from a memory. Similarly, Go-Exploit maintains an archive of states of interest. At the beginning of a self-play trajectory, the start state is either uniformly sampled from the archive or is set to the initial state of the game.

In the games of Connect Four and 9x9 Go, we showed that Go-Exploit exhibits a greater sample efficiency than standard AlphaZero, measured in their average win rates against reference opponents over the course of training and in the results of their head-to-head play. We also compared Go-Exploit to KataGo, a more sample efficient reimplementation of AlphaZero. Go-Exploit's search control strategy resulted in faster learning than KataGo's, however, Go-Exploit's sample efficiency improved when KataGo's other innovations were incorporated.

To learn more about Go-Exploit, please read our [research paper](https://arxiv.org/pdf/2302.12359.pdf), which was accepted to the Proceedings of the 22nd International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2023). In this paper, we further motivate the limitations in AlphaZero's training procedure, we introduce four variants of Go-Exploit using two definitions of *states of interest* and three archive structures, we present experimental results showcasing Go-Exploit's superior sample efficiency relative to AlphaZero and KataGo's search control strategy, and we conclude with a discussion detailing why we believe Go-Exploit learns more efficiently than AlphaZero.

## Implementation:

Go-Exploit and elements of KataGo were coded on top of DeepMind’s [OpenSpiel](https://github.com/deepmind/open_spiel) implementation of AlphaZero. OpenSpiel is an open source repository consisting of environments (games) and popular reinforcement learning and planning algorithms. Most algorithms in OpenSpiel have Python and C++ implementations. Furthermore, some implementations use PyTorch/LibTorch for their neural networks while other implementations use TensorFlow.

We coded Go-Exploit and some elements of KataGo on top of OpenSpiel's C++/LibTorch implementation of AlphaZero. Below is a summary of the relevant files:
- `open_spiel/open_spiel/examples/alpha_zero_torch_example.cc`: Hyperparameter values for AlphaZero and Go-Exploit are defined as flags in this file.
- `open_spiel/open_spiel/algorithms/mcts.cc`: AlphaZero's variant of Monte Carlo Tree Search is implemented in this file. We modified the `SearchNode` struct in `mcts.h` to code some elements of KataGo and to help us collect some statistics. In `mcts.cc`, we modified `MCTSBot::ApplyTreePolicy()` and `MCTSBot::MCTSearch()` to implement KataGo's "Forced Playouts + Policy Target Pruning" and "Playout Cap Randomization". `MCTSBot::MCTSearch()` was further modified to collect search statistics and the set of search states to be added to the archive in Go-Exploit Search States.
- 




Experiments were run using OpenSpiel’s versions of Connect Four and Go.

## Running Our Code:

