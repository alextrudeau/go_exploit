
# Go-Exploit: Targeted Search Control in AlphaZero for Effective Policy Improvement

## Motivation:

AlphaZero is a model-based reinforcement learning (RL) algorithm that has achieved impressive results in two-player, zero-sum games, reaching superhuman play in chess, shogi, and Go. AlphaZero simulates self-play matches with a perfect model of its environment (the rules of the game) to train a neural network that learns a value function and action selection priors over states. Each turn, the value function and priors guide a lookahead search that returns an improved policy. AlphaZero trains its neural network on the self-play matches produced under the improved policies, enabling it to improve its play via policy iteration.

Despite its success, AlphaZero's training suffers from sample inefficiency. In 19x19 Go, AlphaZero requires hundreds of millions of training samples to attain superhuman play. AlphaZero's sample efficiency depends upon the distribution of states visited and trained upon. Although AlphaZero has a perfect model of its environment, it cannot feasibly visit and learn the optimal value for each state. Instead, AlphaZero trains upon the states that it visits on-policy in simulated self-play matches beginning from the initial state of the game. As in other RL algorithms, AlphaZero takes exploratory actions during its self-play matches so that it can train upon a variety of states, enabling it to make more informed action selections in the future. AlphaZero employs simplistic exploration mechanisms during self-play training: randomly perturbing the learned priors guiding search and stochastically selecting actions near the start of self-play matches. As a result, AlphaZero's training procedure exhibits the following limitations:

1. Since AlphaZero begins its self-play matches from the initial state of a game, it often transitions into a terminal state before reaching and exploring states deeper in the game tree. In addition, AlphaZero only samples actions over the first few moves of a self-play match, further limiting exploration deeper in the game tree.
2. AlphaZero's exploration mechanisms cause it to train under weaker, exploratory policies, slowing policy iteration.
3. AlphaZero only produces a single, noisy value target from a full self-play match, slowing value training.

We hypothesized that AlphaZero could address these limitations, and learn with greater sample efficiency, with a more effective search control strategy. Sutton and Barto define search control as "the process that selects the starting states and actions for the simulated experiences generated by the model". In AlphaZero, this would amount to strategically choosing the starting state of its simulated trajectories. We propose one such strategy that adheres to four guiding principles. The algorithm should:

- Continually visit new states throughout the state space to learn their values and a good policy.
- Keep track of states of interest and have the ability to reliably revisit them for further exploration.
- Limit exploration’s bias in the learning targets.
- Produce more independent value targets to train upon.

We introduce Go-Exploit, a novel search control strategy for AlphaZero. Go-Exploit takes inspiration from Go-Explore and Exploring Restart Distributions, which begin simulated episodes from previously visited states sampled from a memory. Similarly, Go-Exploit maintains an archive of states of interest. At the beginning of a self-play trajectory, the start state is either uniformly sampled from the archive or is set to the initial state of the game.

In the games of Connect Four and 9x9 Go, we showed that Go-Exploit exhibits a greater sample efficiency than standard AlphaZero, measured in their average win rates against reference opponents over the course of training and in the results of their head-to-head play. We also compared Go-Exploit to KataGo, a more sample efficient reimplementation of AlphaZero. Go-Exploit's search control strategy resulted in faster learning than KataGo's, however, Go-Exploit's sample efficiency improved when KataGo's other innovations were incorporated.

To learn more about Go-Exploit, please read our [research paper](https://arxiv.org/pdf/2302.12359.pdf), which was accepted to the Proceedings of the 22nd International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2023). In this paper, we further motivate the limitations in AlphaZero's training procedure, we introduce four variants of Go-Exploit using two definitions of *states of interest* and three archive structures, we present experimental results showcasing Go-Exploit's superior sample efficiency relative to AlphaZero and KataGo's search control strategy, and we conclude with a discussion detailing why we believe Go-Exploit learns more efficiently than AlphaZero.

## Implementation:

Go-Exploit and elements of KataGo were coded on top of DeepMind’s [OpenSpiel](https://github.com/deepmind/open_spiel) implementation of AlphaZero. OpenSpiel is an open source repository consisting of environments (games) and popular reinforcement learning and planning algorithms. Most algorithms in OpenSpiel have Python and C++ implementations. Furthermore, some implementations use PyTorch/LibTorch for their neural networks while other implementations use TensorFlow.

We coded Go-Exploit and some elements of KataGo on top of OpenSpiel's C++/LibTorch implementation of AlphaZero. Below is a summary of the relevant files:
- `open_spiel/open_spiel/examples/alpha_zero_torch_example.cc`: Hyperparameter values for AlphaZero and Go-Exploit are defined as flags in this file.
- `open_spiel/open_spiel/algorithms/mcts.cc`: AlphaZero's variant of Monte Carlo Tree Search is implemented in this file. We added members to the `SearchNode` struct in `mcts.h` to help us keep track of some search statistics, to help us implement some elements of KataGo, and to help us keep track of search states for Go-Exploit Search States. In `mcts.cc`, we modified `MCTSBot::ApplyTreePolicy()` and `MCTSBot::MCTSearch()` to implement KataGo's "Forced Playouts + Policy Target Pruning" and "Playout Cap Randomization". `MCTSBot::MCTSearch()` was further modified to collect search statistics and the set of search states to be added to the archive in Go-Exploit Search States.
- `open_spiel/open_spiel/algorithms/alpha_zero_torch/model.cc`: `model.h` and `model.cc` define the functions used to build the policy-value network using Libtorch.
- `open_spiel/open_spiel/algorithms/alpha_zero_torch/device_manager.h`: This file defines the `DeviceManager()` class which is used to determine which gpu should be used to run an inference.
- `open_spiel/open_spiel/algorithms/alpha_zero_torch/vpevaluator.cc`: This file defines the functions that are used to process inference calls from `MCTSBot::MCTSearch()` in `mcts.cc`. If a state has already been evaluated during the given learning step, `cache_` returns the saved value and priors. Otherwise, `VPNetEvaluator::Runner()` forms inference batches of states and passes them to a gpu for inference.
- `open_spiel/open_spiel/algorithms/alpha_zero_torch/vpnet.cc`: This file defines the `VPNetModel::Inference()` and `VPNetModel::Learn()` functions which convert inference and training batches into input tensors for the policy-value network. We modified the way in which data was transferred between the cpu and gpu in order to increase the speed of execution.
- `open_spiel/open_spiel/algorithms/alpha_zero_torch/alpha_zero.cc`: `AlphaZero()` initializes a new policy-value network or loads an existing checkpoint and then makes a copy of it for each gpu. A trajectory queue and archive are also created. `AlphaZero()` then initializes `training_actor()` threads, `archive_actor()` threads, and a `learner()` thread. `training_actor()` threads call `PlayGame()` to generate self-play matches. Completed self-play trajectories are pushed on to the trajectory queue. The `learner()` thread pops self-play trajectories off of the trajectory queue and adds the new training samples to the experience replay buffer. Once a quota of newly generated training samples is reached, `learner()` samples a training batch from the experience replay buffer and updates the policy-value network. `archive_actor()` threads concurrently call `PlayGame()` and generate self-play matches beginning from the initial state of the game. Search states observed during these self-play matches are used to populate the archive in Go-Exploit Search States.

Experiments were run using OpenSpiel’s versions of Connect Four and Go.

## Running Our Code:

**Prerequisites**:
1. Ubuntu 20.04
2. GCC 9
3. CUDA enabled GPUs
4. CUDA 11.0 and CUDNN installed

**Installation Steps**:
1. `cd open_spiel`
2. `export OPEN_SPIEL_BUILD_WITH_LIBTORCH=ON`
3. `export OPEN_SPIEL_BUILD_WITH_LIBNOP=ON`
4. `export OPEN_SPIEL_BUILD_WITH_LIBTORCH_DOWNLOAD_URL=https://download.pytorch.org/libtorch/cu110/libtorch-cxx11-abi-shared-with-deps-1.7.1%2Bcu110.zip`
5. `./install.sh`
6. `mkdir build`
7. `cd build`
8. `CXX=g++-9 cmake -DPython3_EXECUTABLE=$(which python3) -DCMAKE_CXX_COMPILER=${CXX} ../open_spiel`
9. `make -j$(nproc)`
10. `cd examples`

**Running Connect Four**:
`nohup ./alpha_zero_torch_example --path=/tmp --max_steps=600 --use_search_states=true --archive_type=Circular --max_archive_size=1000000 --devices=cuda:0,cuda:1,cuda:2,cuda:3 --inference_threads=8 --uct_c=1.0 --policy_alpha=1.0 --policy_epsilon=0.25 --start_state_prob=0.01 --temperature_drop=10 --weight_decay=1e-05 --learning_rate=0.001 --training_actors=700 --archive_actors=50 --evaluators=50 --katago_init=false --position_branching=false --game_branching=false --playout_cap=false --use_forced_playouts=false --write_search_stats=true &`

**Running 9x9 Go**:
`nohup ./alpha_zero_torch_example --path=/tmp --max_steps=900 --use_search_states=false --archive_type=Expanding --max_archive_size=1000000 --devices=cuda:0,cuda:1,cuda:2,cuda:3 --inference_threads=8 --uct_c=2.0 --policy_alpha=0.03 --policy_epsilon=0.1 --start_state_prob=0.1 --temperature_drop=2 --weight_decay=1e-05 --learning_rate=0.001 --training_actors=700 --archive_actors=50 --evaluators=50 --katago_init=false --position_branching=false --game_branching=false --playout_cap=false --use_forced_playouts=false --write_search_stats=true &`

