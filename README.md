
# Go-Exploit: Targeted Search Control in AlphaZero for Effective Policy Improvement

## Motivation:

AlphaZero is a model-based reinforcement learning (RL) algorithm that has achieved impressive results in two-player, zero-sum games, reaching superhuman play in chess, shogi, and Go. AlphaZero simulates self-play matches with a perfect model of its environment (the rules of the game) to train a neural network that learns a value function and action selection priors over states. Each turn, the value function and priors guide a lookahead search that returns an improved policy. AlphaZero trains its neural network on the self-play matches produced under the improved policies, enabling it to improve its play via policy iteration.

Despite its success, AlphaZero's training suffers from sample inefficiency. In 19x19 Go, AlphaZero requires hundreds of millions of training samples to attain superhuman play. AlphaZero's sample efficiency depends upon the distribution of states visited and trained upon. Although AlphaZero has a perfect model of its environment, it cannot feasibly visit and learn the optimal value for each state. Instead, AlphaZero trains upon the states that it visits on-policy in simulated self-play matches beginning from the initial state of the game. As in other RL algorithms, AlphaZero takes exploratory actions during its self-play matches so that it can train upon a variety of states, enabling it to make more informed action selections in the future. AlphaZero employs simplistic exploration mechanisms during self-play training: randomly perturbing the learned priors guiding search and stochastically selecting actions near the start of self-play matches. As a result, AlphaZero's training procedure exhibits the following limitations:

1. Since AlphaZero begins its self-play matches from the initial state of a game, it often transitions into a terminal state before reaching and exploring states deeper in the game tree. In addition, AlphaZero only samples actions over the first few moves of a self-play match, further limiting exploration deeper in the game tree.
2. AlphaZero's exploration mechanisms cause it to train under weaker, exploratory policies, slowing policy iteration.
3. AlphaZero only produces a single, noisy value target from a full self-play match, slowing value training.

We hypothesized that AlphaZero could address these limitations, and learn with greater sample efficiency, with a more effective search control strategy. Sutton and Barto define search control as "the process that selects the starting states and actions for the simulated experiences generated by the model". In AlphaZero, this would amount to strategically choosing the starting state of its simulated trajectories. We propose one such strategy that adheres to four guiding principles. The algorithm should:

- Continually visit new states throughout the state space to learn their values and a good policy.
- Keep track of states of interest and have the ability to reliably revisit them for further exploration.
- Limit exploration’s bias in the learning targets.
- Produce more independent value targets to train upon.

We introduce Go-Exploit, a novel search control strategy for AlphaZero. Go-Exploit takes inspiration from Go-Explore and Exploring Restart Distributions, which begin simulated episodes from previously visited states sampled from a memory. Similarly, Go-Exploit maintains an archive of states of interest. At the beginning of a self-play trajectory, the start state is either uniformly sampled from the archive or is set to the initial state of the game.

In the games of Connect Four and 9x9 Go, we showed that Go-Exploit exhibits a greater sample efficiency than standard AlphaZero, measured in their average win rates against reference opponents over the course of training and in the results of their head-to-head play. We also compared Go-Exploit to KataGo, a more sample efficient reimplementation of AlphaZero. Go-Exploit's search control strategy resulted in faster learning than KataGo's, however, Go-Exploit's sample efficiency improved when KataGo's other innovations were incorporated.

To learn more about Go-Exploit, please read our [research paper](https://arxiv.org/pdf/2302.12359.pdf), which was accepted to the Proceedings of the 22nd International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2023). In this paper, we further motivate the limitations in AlphaZero's training procedure, we introduce four variants of Go-Exploit using two definitions of *states of interest* and three archive structures, we present experimental results showcasing Go-Exploit's superior sample efficiency relative to AlphaZero and KataGo's search control strategy, and we conclude with a discussion detailing why we believe Go-Exploit learns more efficiently than AlphaZero.

## Implementation:

Go-Exploit and elements of [KataGo](https://arxiv.org/abs/1902.10565) were coded on top of DeepMind’s [OpenSpiel](https://github.com/deepmind/open_spiel) implementation of AlphaZero. OpenSpiel is an open source repository consisting of environments (games) and popular reinforcement learning and planning algorithms. Most algorithms in OpenSpiel have Python and C++ implementations. Furthermore, some implementations use PyTorch/LibTorch for their neural networks while other implementations use TensorFlow.

We coded Go-Exploit and some elements of KataGo on top of OpenSpiel's C++/LibTorch implementation of AlphaZero. Below is a summary of the relevant files:

### `open_spiel/open_spiel/examples/alpha_zero_torch_example.cc`:

Hyperparameter values for AlphaZero, Go-Exploit, and KataGo are defined as flags in this file. Below is a description of the important flags:
* `--game`: The name of the game that AlphaZero learns to play. Use "connect_four" for Connect Four and "go" for 9x9 Go. In the `AlphaZero()` function in `alpha_zero.cc`, the code is configured such that when `--game=go`, a 9x9 board is used.
* `--path`: Where the output files, logs, and neural network checkpoints are written to.
* `--nn_model`: The neural network architecture used for the policy-value network. Use `--nn_model=resnet` to use the standard residual neural network.
* `--nn_width`: The number of filters in a residual block.
* `--nn_depth`: The number of residual blocks in the policy-value network.
* `--uct_c`: The $c_{\textrm{puct}}$ constant used in search.
* `--eval_uct_c`: The $c_{\textrm{puct}}$ constant used in search by MCTS-Solver in the evaluation matches.
* `--temperature`: The Softmax temperature $\tau$ used when converting the search visit counts into the policy $\mathbf{\pi}_t$.
* `--temperature_drop`: The number of action sampling moves $k$ in a self-play trajectory. In the first $k$ moves, an action is sampled from $\mathbf{\pi}_t$. After the first $k$ moves, the action with the most search visits is played.
* `--learning_rate`: The learning rate $lr$ used in the policy-value network's SGD updates.
* `--weight_decay`: The regularization constant $c$ used in AlphaZero's loss function.
* `--policy_alpha`: The parameter $\alpha$ that parameterizes the Dirichlet distribution for the Dirichlet noise.
* `--policy_epsilon`: The parameter $\epsilon$ that controls the magnitude of the Dirichlet noise.
* `--replay_buffer_size`: The capacity of the circular experience replay buffer.
* `--replay_buffer_reuse`: The number of learning steps for which a training sample remains in the experience replay buffer.
* `--checkpoint_freq`: How often a checkpoint of the policy-value network is saved and written to `--path`.
* `--max_simulations`: The number of search iterations performed each turn. When `--playout_cap=true` and Playout Cap Randomization is performed, "full searches" use `--max_simulations` search iterations.
* `--train_batch_size`: The size of the sampled training batches used to update the policy-value network.
* `--inference_batch_size`: The number of states that are passed to a gpu at a time to run inference.
* `--inference_threads`: The number of threads passing inference batches from the cpu to gpus.
* `--inference_cache`: The size of the cache that stores state values and priors for states visited in the given learning step.
* `--devices`: The devices where copies of the policy-value network are loaded to run inference. If no gpus are available, use `--devices=cpu`. If gpus are available, create a comma separated list using the convention "cuda:n", where "n" is nth gpu. For example, if four gpus were available, you would use `--devices=cuda:0,cuda:1,cuda:2,cuda:3`.
* `--training_actors`: The number of training actor threads generating self-play matches to train the policy-value network.
* `--archive_actors`: The number of archive actor threads generating self-play matches to populate the archive for Go-Exploit Search States.
* `--evaluators`: The number of evaluator threads playing evaluation matches against MCTS-Solver. The results of these matches are written to `learner.jsonl`.
* `--eval_levels`: The number of "difficulty levels" of MCTS-Solver the evaluator threads play against. For example, if `--eval_levels=2`, evaluation matches are played against versions of MCTS-Solver with `max_simulations*10^(n/2)` search iterations where `n in range(eval_levels)`.
* `--max_steps`: The number of learning steps over which the policy-value network is trained.
* `--evaluation_window`: The number of learning steps over which evaluation match results are averaged when reporting results to `learner.jsonl`
* `--start_state_prob`: The probability $\lambda$ of beginning self-play trajectories from $s_0$ in Go-Exploit. The Go-Exploit algorithm is only performed if `start_state_prob > 0.0`.
* `--use_search_states`: If `--use_search_states=true`, Go-Exploit uses the "Go-Exploit Search States" definition for *states of interest*. In Go-Exploit Search States, the archive is populated with search states observed by the archive actors in self-play matches played from $s_0$. If `--use_search_states=false`, Go-Exploit uses the "Go-Exploit Visited States" definition for *states of interest*. In Go-Exploit Visited States, the archive is populated with states visited by the training actors.
* `--archive_type`: The type of archive used by Go-Exploit. If `--archive_type=Circular`, a fixed-size circular archive is used. If `--archive_type=Reservoir`, a fixed-size archive is updated using Reservoir Sampling. If `--archive_type=Expanding`, an expanding archive consisting of every state of interest is used.
* `--max_archive_size`: The capacity of a fixed-size archive (for Circular and Reservoir archives).
* `--katago_init`: When `--katago_init=true`, self-play trajectories are initialized using KataGo's trajectory initialization scheme. You can learn more in [Appendix D](https://arxiv.org/pdf/1902.10565.pdf).
* `--position_branching`: When `--position_branching=true`, KataGo's position branching scheme is used. KataGo branches from previously visited states in 2.5% of positions and tries an alternative action. You can learn more in [Appendix D](https://arxiv.org/pdf/1902.10565.pdf).
* `--game_branching`: When `--game_branching=true`, KataGo's game branching scheme is used. KataGo branches from an early position in a self-play match in 5% of matches. You can learn more in [Appendix D](https://arxiv.org/pdf/1902.10565.pdf).
* `--board_width`: The width of the game board. For example, `--board_width=9` in 9x9 Go and `--board_width=7` in Connect Four.
* `--board_length`: The length of the game board. For example, `--board_width=9` in 9x9 Go and `--board_width=6` in Connect Four.
* `--playout_cap`: When `--playout_cap=true`, KataGo's Playout Cap Randomization is performed. You can learn more in [Section 3.1](https://arxiv.org/pdf/1902.10565.pdf).
* `--full_search_prob`: The probability of performing a "full search" in KataGo's Playout Cap Randomization.
* `--fast_search`: The number of search iterations performed in a "fast search" in KataGo's Playout Cap Randomization.
* `--use_forced_playouts`: When `--use_forced_playouts=true`, KataGo's Forced Playouts + Policy Target Pruning are performed. You can learn more in [Section 3.2](https://arxiv.org/pdf/1902.10565.pdf).
* `--write_search_stats`: When `--write_search_stats=true`, search statistics are written to a file.

### `open_spiel/open_spiel/algorithms/mcts.cc`:

AlphaZero's variant of Monte Carlo Tree Search is implemented in this file. We added members to the `SearchNode` struct in `mcts.h` to keep track of some search statistics, to implement some elements of KataGo, and to keep track of search states for Go-Exploit Search States. In `mcts.cc`, we modified `MCTSBot::ApplyTreePolicy()` and `MCTSBot::MCTSearch()` to implement KataGo's "Forced Playouts + Policy Target Pruning" and "Playout Cap Randomization". `MCTSBot::MCTSearch()` was further modified to collect search statistics and the set of search states to be added to the archive in Go-Exploit Search States.

### `open_spiel/open_spiel/algorithms/alpha_zero_torch/model.cc`:

`model.h` and `model.cc` define the functions used to build the policy-value network using Libtorch.

### `open_spiel/open_spiel/algorithms/alpha_zero_torch/device_manager.h`:

This file defines the `DeviceManager()` class which is used to determine which gpu should be used to run an inference.

### `open_spiel/open_spiel/algorithms/alpha_zero_torch/vpevaluator.cc`:

This file defines the functions that are used to process inference calls from `MCTSBot::MCTSearch()` in `mcts.cc`. If a state has already been evaluated during the given learning step, `cache_` returns the saved value and priors. Otherwise, `VPNetEvaluator::Runner()` forms inference batches of states and passes them to a gpu for inference.

### `open_spiel/open_spiel/algorithms/alpha_zero_torch/vpnet.cc`:

This file defines the `VPNetModel::Inference()` and `VPNetModel::Learn()` functions which convert inference and training batches into input tensors for the policy-value network. We modified the way in which data was transferred between the cpu and gpu in order to increase the speed of execution.

### `open_spiel/open_spiel/algorithms/alpha_zero_torch/alpha_zero.cc`:

Go-Exploit's Archive class `Archive::Archive`is defined in `alpha_zero.h` and `alpha_zero.cc`. In the constructor, `Archive::Archive()`, the archive is initialized with the initial state of the game $s_0$. The `Archive::Sample()` method samples a state from the archive uniformly at random. The `Archive::Update()` method adds newly visited/observed states to the archive.

The `UpdateStates` class keeps track of which states have been used to update the policy-value network. This class is only used for collecting statistics. The current version of the code does not use this class.

The `StartInfoFromLearnerJson()` function enables a run to be continued from a checkpoint.

The `Trajectory` and `State` structs keep track of information required for updating the policy-value network as well as the search states needed for Go-Exploit Search States. These structs also keep track of search statistics.

The `KataGoInitializeGame()` function implements KataGo's trajectory initialization scheme. It returns a `KataGoInitData` object consisting of the data pertaining to the sampled initial state. `KataGoRecursiveBranch()` is a function implementing KataGo's position branching scheme. `KataGoBranch()` is a function implementing KataGo's game branching scheme.

The `PlayGame()` function is used to generate self-play trajectories by the training actors and archive actors during training. The evaluator threads also call `PlayGame()` to play out evaluation matches against MCTS-Solver.

`InitAZBot()` is a function that creates an instantiation of AlphaZero's variant of MCTS.

`training_actor()` is a function that continuously generates self-play trajectories that are transferred to the `learner()`'s experience replay buffer and are used to train the policy-value network.

`archive_actor()` is a function that continuously generates self-play trajectories beginning from $s_0$. If `use_search_states=true`, the search states observed are added to the archive.

The `EvalResults` class processes the evaluation match results against MCTS-Solver.

`evaluator()` is a function that continuously plays evaluation matches against MCTS-Solver using the current policy-value network.

`learner()` is a function that produces training samples from the self-play trajectories produced by the training actors and then adds them to the experience replay buffer. Once `replay_buffer_size / replay_buffer_reuse` new training samples have been produced, `learner()` samples training batches from the experience replay buffer and updates the policy-value network. If configured for Go-Exploit Visited States, `learner()` adds the newly visited states to the archive.

`AlphaZero()` initializes a new policy-value network or loads an existing checkpoint and then makes a copy of it for each gpu. A trajectory queue and archive are also created. `AlphaZero()` then initializes `training_actor()` threads, `archive_actor()` threads, and a `learner()` thread.

Experiments were run using OpenSpiel’s versions of Connect Four and Go.

## Running Our Code:

Below are the environment and installation steps we used. Our code can likely be run with Ubuntu 22.04 and CUDA 11.7 and lower. We recommend first deciding which version of Libtorch will be used (currently CUDA 11.7 and lower are supported) and then setting up your environment with the corresponding versions of Ubuntu, CUDA, and CUDNN. The installation steps provided by [OpenSpiel](https://github.com/deepmind/open_spiel) may also be useful.

**Prerequisites**:
1. Ubuntu 20.04 (we used 20.04.3 LTS)
2. Kernel: 5.4.0-86-generic
3. GCC 9 (we used 9.3.0)
4. CUDA enabled GPUs (we used four NVIDIA Tesla P4 gpus)
5. CUDA 11.0 and CUDNN 8.0.5 installed
6. NVIDIA Driver: 450.51.06

**Installation Steps**:
1. `cd go_exploit`
2. `export OPEN_SPIEL_BUILD_WITH_LIBTORCH=ON`
3. `export OPEN_SPIEL_BUILD_WITH_LIBNOP=ON`
4. `export OPEN_SPIEL_BUILD_WITH_LIBTORCH_DOWNLOAD_URL=https://download.pytorch.org/libtorch/cu110/libtorch-cxx11-abi-shared-with-deps-1.7.1%2Bcu110.zip`
5. `./install.sh`
6. `mkdir build`
7. `cd build`
8. `CXX=g++-9 cmake -DPython3_EXECUTABLE=$(which python3) -DCMAKE_CXX_COMPILER=${CXX} ../open_spiel`
9. `make -j$(nproc)`
10. `cd examples`

**Running Connect Four**:
```
nohup ./alpha_zero_torch_example --path=/tmp --max_steps=600 --use_search_states=true --archive_type=Circular --max_archive_size=1000000 --devices=cuda:0,cuda:1,cuda:2,cuda:3 --inference_threads=8 --uct_c=1.0 --policy_alpha=1.0 --policy_epsilon=0.25 --start_state_prob=0.01 --temperature_drop=10 --weight_decay=1e-05 --learning_rate=0.001 --training_actors=700 --archive_actors=50 --evaluators=50 --katago_init=false --position_branching=false --game_branching=false --playout_cap=false --use_forced_playouts=false --write_search_stats=true &
```

**Running 9x9 Go**:
```
nohup ./alpha_zero_torch_example --path=/tmp --max_steps=900 --use_search_states=false --archive_type=Expanding --max_archive_size=1000000 --devices=cuda:0,cuda:1,cuda:2,cuda:3 --inference_threads=8 --uct_c=2.0 --policy_alpha=0.03 --policy_epsilon=0.1 --start_state_prob=0.1 --temperature_drop=2 --weight_decay=1e-05 --learning_rate=0.001 --training_actors=700 --archive_actors=50 --evaluators=50 --katago_init=false --position_branching=false --game_branching=false --playout_cap=false --use_forced_playouts=false --write_search_stats=true &
```

