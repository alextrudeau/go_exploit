// Copyright 2019 DeepMind Technologies Ltd. All rights reserved.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

#include "open_spiel/algorithms/alpha_zero_torch/alpha_zero.h"

#include <algorithm>
#include <cstdlib>
#include <iostream>
#include <memory>
#include <random>
#include <string>
#include <utility>
#include <vector>

#include "open_spiel/abseil-cpp/absl/algorithm/container.h"
#include "open_spiel/abseil-cpp/absl/random/uniform_real_distribution.h"
#include "open_spiel/abseil-cpp/absl/strings/str_cat.h"
#include "open_spiel/abseil-cpp/absl/strings/str_join.h"
#include "open_spiel/abseil-cpp/absl/strings/str_split.h"
#include "open_spiel/abseil-cpp/absl/synchronization/mutex.h"
#include "open_spiel/abseil-cpp/absl/time/clock.h"
#include "open_spiel/abseil-cpp/absl/time/time.h"
#include "open_spiel/algorithms/alpha_zero_torch/device_manager.h"
#include "open_spiel/algorithms/alpha_zero_torch/vpevaluator.h"
#include "open_spiel/algorithms/alpha_zero_torch/vpnet.h"
#include "open_spiel/algorithms/mcts.h"
#include "open_spiel/spiel.h"
#include "open_spiel/spiel_utils.h"
#include "open_spiel/utils/circular_buffer.h"
#include "open_spiel/utils/data_logger.h"
#include "open_spiel/utils/file.h"
#include "open_spiel/utils/json.h"
#include "open_spiel/utils/logger.h"
#include "open_spiel/utils/lru_cache.h"
#include "open_spiel/utils/serializable_circular_buffer.h"
#include "open_spiel/utils/stats.h"
#include "open_spiel/utils/thread.h"
#include "open_spiel/utils/threaded_queue.h"

#include <unordered_map>
#include <mutex>
#include <chrono>
#include <fstream>
#include <sstream>

namespace open_spiel {
namespace algorithms {
namespace torch_az {

absl::Mutex eval_mutex_;

// Either An Expanding Archive Or A Fixed Size Archive That Replaces The Oldest Entry With A New Entry
Archive::Archive(std::string archive_type, int max_size, unsigned seed, std::string path)
  : archive_type_(archive_type),
    fixed_size_archive_(max_size),
    max_size_(max_size),
    rng_(seed),
    reservoir_index_(max_size),
    path_(path) {

  std::cout << "Initializing Archive" << std::endl;

  std::vector<open_spiel::Action> action_sequence;
  
  ArchiveEntry initial_state_entry = {action_sequence, 0};

  // Fixed-Size Circular Archive uses the SerializableCircularBuffer
  // Fixed-Size Archive with Reservoir Sampling represents archive as a std::vector
  // Expanding Archive also represents archive as a std::vector
  if (archive_type_ == "Circular") {
    fixed_size_archive_.Add(initial_state_entry);
  }
  else {
    expanding_archive_.push_back(initial_state_entry);
    weights_.push_back(1.0);
    current_size_ = 1;
  }
}

// Samples a state of interest from the archive to be the start state of a self-play trajectory
ArchiveEntry Archive::Sample() {
  absl::MutexLock lock(&archive_mutex_);

  if (archive_type_ == "Circular") {
    std::vector<ArchiveEntry> sampled_entry = fixed_size_archive_.Sample(&rng_, 1);
    return sampled_entry[0];
  }
  else {
    std::discrete_distribution<int> archive_dist(weights_.begin(), weights_.end());
    int state_index = archive_dist(rng_);
    return expanding_archive_[state_index];
  }
}

// Adds new states of interest to the archive
void Archive::Update(std::vector<ArchiveEntry>& step_archive) {
  absl::MutexLock lock(&archive_mutex_);
  
  for (ArchiveEntry entry : step_archive) {
    if (archive_type_ == "Circular") {
      fixed_size_archive_.Add(entry);
    }
    else if (archive_type_ == "Reservoir") {
      if (current_size_ < max_size_) {
        weights_.push_back(1.0);
        expanding_archive_.push_back(entry);
        current_size_ += 1;
      }
      else {
        // Reservoir Sampling
        std::uniform_int_distribution<int> uni(0, reservoir_index_);
        int random_integer = (int) uni(rng_);

        if (random_integer < max_size_) {
          // Replace existing entry with new entry
          double weight = 1.0;
          weights_[random_integer] = weight;
          expanding_archive_[random_integer] = entry;
        }
        reservoir_index_ += 1;
      }
    }
    else {
      weights_.push_back(1.0);
      expanding_archive_.push_back(entry);
    }
  }

  if (archive_type_ == "Circular") {
    std::cout << "States in Archive: " << std::to_string(fixed_size_archive_.Size()) << std::endl;
  }
  else {
    std::cout << "States in Archive: " << std::to_string(expanding_archive_.size()) << std::endl;
  }
}

void UpdateStates::Update(std::vector<VPNetModel::TrainInputs> learn_inputs) {
  absl::MutexLock lock(&mutex_);
  for (VPNetModel::TrainInputs train_input : learn_inputs) {
    std::string train_input_str = train_input.state_str;
    if (update_states_.find(train_input_str) == update_states_.end()) {
      update_states_[train_input_str] = 1;
    }
    else{
      update_states_[train_input_str] += 1;
    }
  }
}

int UpdateStates::GetUpdateCount(std::string state_str) {
  absl::MutexLock lock(&mutex_);
  if (update_states_.find(state_str) == update_states_.end()) {
    return 0;
  }
  else {
    return update_states_[state_str];
  }
}

void UpdateStates::Write(std::string path, int step) {
  absl::MutexLock lock(&mutex_);
  std::ofstream update_states_file;
  update_states_file.open(path + "/Update_States_Step_" + std::to_string(step) + ".csv", std::ios::out | std::ios::app);
  for( auto& entry : update_states_ ) {
    std::string state_str = entry.first;
    int model_updates = entry.second;

    update_states_file << state_str << "," << std::to_string(model_updates) << std::endl;
  }
  update_states_file.close();
}

struct StartInfo {
  absl::Time start_time;
  int start_step;
  int model_checkpoint_step;
  int64_t total_trajectories;
};

StartInfo StartInfoFromLearnerJson(const std::string& path) {
  StartInfo start_info;
  file::File learner_file(path + "/learner.jsonl", "r");
  std::vector<std::string> learner_lines = absl::StrSplit(
      learner_file.ReadContents(), "\n");
  std::string last_learner_line;

  // Get the last non-empty line in learner.jsonl.
  for (int i = learner_lines.size() - 1; i >= 0; i--) {
    if (!learner_lines[i].empty()) {
      last_learner_line = learner_lines[i];
      break;
    }
  }

  json::Object last_learner_json = json::FromString(
      last_learner_line).value().GetObject();

  start_info.start_time = absl::Now() - absl::Seconds(
      last_learner_json["time_rel"].GetDouble());
  start_info.start_step = last_learner_json["step"].GetInt() + 1;
  start_info.model_checkpoint_step = VPNetModel::kMostRecentCheckpointStep;
  start_info.total_trajectories =
      last_learner_json["total_trajectories"].GetInt();

  return start_info;
}

struct Trajectory {
  struct State {
    std::vector<float> observation;
    open_spiel::Player current_player;
    std::vector<open_spiel::Action> legal_actions;
    open_spiel::Action action;
    open_spiel::ActionsAndProbs policy;
    double value;
    std::string state_str;
    std::vector<open_spiel::Action> action_sequence;
    std::unordered_map<std::string, SearchArchiveEntry> search_states;
    int depth;
    bool full_search;
    bool is_mcts_solver;
    std::unordered_map<int, int> search_visits;
    std::unordered_map<int, double> search_priors;
    std::unordered_map<int, double> search_action_values;
    std::unordered_map<int, int> search_model_update_states;
    std::unordered_map<int, int> search_model_updates;
    std::unordered_map<int, int> search_terminal_states;
    std::unordered_map<int, int> search_nonterminal_states;
  };

  std::vector<State> states;
  std::vector<double> returns;
};

KataGoInitData KataGoInitializeGame(const open_spiel::Game& game, int init_moves,
                                    std::shared_ptr<VPNetEvaluator> vp_eval, std::mt19937* rng) {
  std::shared_ptr<open_spiel::State> state = game.NewInitialState();
  std::vector<open_spiel::Action> action_sequence;

  std::shared_ptr<open_spiel::State> previous_state;
  for (int i = 0; i < init_moves; ++i) {
    previous_state = state->Clone();
    open_spiel::ActionsAndProbs policy = vp_eval->Prior(*state);
    NormalizePolicy(&policy);
    open_spiel::Action action = open_spiel::SampleAction(policy, *rng).first;
    state->ApplyAction(action);

    if (state->IsTerminal()) {
      std::cout << "KataGo Init State Is Terminal. Returning Previous State" << std::endl;
      KataGoInitData katago_init_data = {previous_state, action_sequence, i};
      return katago_init_data;
    }
    else {
      action_sequence.push_back(action);
    }
  }

  KataGoInitData katago_init_data = {state, action_sequence, init_moves};

  return katago_init_data;
}

// 2.5% of positions are branched to try an alternative move sampled from the policy output by the policy-value network
int KataGoRecursiveBranch(const AlphaZeroConfig& config, std::shared_ptr<open_spiel::State> state, std::vector<open_spiel::Action> action_sequence,
                    int depth, std::shared_ptr<VPNetEvaluator> vp_eval, std::vector<std::unique_ptr<MCTSBot>>* bots,
                    std::mt19937* rng, open_spiel::Action banned_action, ThreadedQueue<Trajectory>* trajectory_queue,
                    UpdateStates& update_states) {
  Trajectory trajectory;

  absl::uniform_real_distribution<double> dist(0.0, 1.0);
  double rand_num = dist(*rng);

  open_spiel::ActionsAndProbs legal_actions = vp_eval->Prior(*state);
  open_spiel::ActionsAndProbs policy;

  for (int i = 0; i < legal_actions.size(); i++) {
    if (legal_actions[i].first != banned_action) {
      if (rand_num < 0.7) {
        policy.emplace_back(legal_actions[i].first, std::pow(legal_actions[i].second, 1.0));
      }
      else if (rand_num < 0.95) {
        // Unclear whether it should be 2.0 or 0.5 (0.5 makes the policy more uniform)
        policy.emplace_back(legal_actions[i].first, std::pow(legal_actions[i].second, 0.5));
      }
      else {
        // Each action has equal probability when temperature equals infinity
        policy.emplace_back(legal_actions[i].first, 1.0);
      }
    }
  }

  if (policy.size() > 0) {
    NormalizePolicy(&policy);
  }

  open_spiel::Action action;
  if (policy.size() > 0) {
    if (rand_num < 0.95) {
      action = open_spiel::SampleAction(policy, *rng).first;
    }
    else {
      std::shuffle(policy.begin(), policy.end(), *rng);
      action = policy[0].first;
    }
  }
  else {
    action = banned_action;
  }

  state->ApplyAction(action);
  depth += 1;
  action_sequence.push_back(action);

  std::string state_str = state->ToString();
  state_str.erase(std::remove(state_str.begin(), state_str.end(), '\n'), state_str.end());

  // Only want to train on and recursively continue from non-terminal states
  if (state->IsTerminal()){
    return 0;
  }
  else {
    // Perform full search to create policy and value target
    open_spiel::Player player = state->CurrentPlayer();
    std::unordered_map<std::string, SearchArchiveEntry> search_states;
    std::unique_ptr<SearchNode> root = (*bots)[player]->MCTSearch(*state, action_sequence, search_states, config.use_search_states, depth, config.max_simulations, true, config.use_forced_playouts);

    open_spiel::ActionsAndProbs search_policy;
    search_policy.reserve(root->children.size());
    std::unordered_map<int, int> search_visits;
    std::unordered_map<int, double> search_priors;
    std::unordered_map<int, double> search_action_values;
    std::unordered_map<int, int> search_model_update_states;
    std::unordered_map<int, int> search_model_updates;
    std::unordered_map<int, int> search_terminal_states;
    std::unordered_map<int, int> search_nonterminal_states;
    for (const SearchNode& c : root->children) {
      search_policy.emplace_back(c.action, std::pow(c.explore_count, 1.0 / config.temperature));

      if (config.write_search_stats) {
        // Update Search Stats
        search_visits[(int) c.action] = c.explore_count;
        search_priors[(int) c.action] = c.prior;
        search_action_values[(int) c.action] = c.total_reward / c.explore_count;
        search_model_update_states[(int) c.action] = 0;
        search_model_updates[(int) c.action] = 0;
        search_terminal_states[(int) c.action] = c.terminal_states;
        search_nonterminal_states[(int) c.action] = c.nonterminal_states;

        for (std::string subtree_state_str : c.subtree_states) {
          int state_updates = update_states.GetUpdateCount(subtree_state_str);
          if (state_updates > 0) {
            search_model_update_states[(int) c.action] += 1;
            search_model_updates[(int) c.action] += state_updates;
          }
        }
      }
    }

    NormalizePolicy(&search_policy);

    double root_value = root->total_reward / root->explore_count;
    trajectory.states.push_back(Trajectory::State{
      state->ObservationTensor(), player, state->LegalActions(), action, std::move(search_policy), root_value, state_str, action_sequence, search_states, depth, true, (*bots)[player]->IsMCTSSolver(), search_visits, search_priors, search_action_values, search_model_update_states, search_model_updates, search_terminal_states, search_nonterminal_states});
    trajectory.returns.resize(2);
    trajectory.returns[player] = root_value;
    trajectory.returns[1 - player] = -root_value;
    trajectory_queue->Push(trajectory, absl::Seconds(10));

    // Sample new random number to see if we continue recursion
    rand_num = dist(*rng);
    if (rand_num < 0.25) {
      return KataGoRecursiveBranch(config, state, action_sequence, depth, vp_eval, bots, rng, kInvalidAction, trajectory_queue, update_states);
    }
    return 0;
  }
}

// In 5% of games, the game is branched after the first r moves
Trajectory KataGoBranch(const AlphaZeroConfig& config, std::shared_ptr<open_spiel::State> state,
                    std::vector<open_spiel::Action> action_sequence, int depth, std::vector<std::unique_ptr<MCTSBot>>* bots,
                    std::mt19937* rng, double cutoff_value, UpdateStates& update_states) {
  Trajectory trajectory;

  while (true) {
    std::string state_str = state->ToString();
    state_str.erase(std::remove(state_str.begin(), state_str.end(), '\n'), state_str.end());

    open_spiel::Player player = state->CurrentPlayer();
    std::unordered_map<std::string, SearchArchiveEntry> search_states;

    // Playout Cap Randomization
    int search_sims = config.max_simulations;
    bool full_search = true;
    if (config.playout_cap) {
      absl::uniform_real_distribution<double> dist(0.0, 1.0);
      double cap_rand_num = dist(*rng);
      if (cap_rand_num > config.full_search_prob) {
        search_sims = config.fast_search;
        full_search = false;
      }
    }

    std::unique_ptr<SearchNode> root = (*bots)[player]->MCTSearch(*state, action_sequence, search_states, config.use_search_states, depth, search_sims, full_search, config.use_forced_playouts);

    open_spiel::ActionsAndProbs policy;
    policy.reserve(root->children.size());
    std::unordered_map<int, int> search_visits;
    std::unordered_map<int, double> search_priors;
    std::unordered_map<int, double> search_action_values;
    std::unordered_map<int, int> search_model_update_states;
    std::unordered_map<int, int> search_model_updates;
    std::unordered_map<int, int> search_terminal_states;
    std::unordered_map<int, int> search_nonterminal_states;
    for (const SearchNode& c : root->children) {
      policy.emplace_back(c.action, std::pow(c.explore_count, 1.0 / config.temperature));

      if (config.write_search_stats) {
        // Update Search Stats
        search_visits[(int) c.action] = c.explore_count;
        search_priors[(int) c.action] = c.prior;
        search_action_values[(int) c.action] = c.total_reward / c.explore_count;
        search_model_update_states[(int) c.action] = 0;
        search_model_updates[(int) c.action] = 0;
        search_terminal_states[(int) c.action] = c.terminal_states;
        search_nonterminal_states[(int) c.action] = c.nonterminal_states;

        for (std::string subtree_state_str : c.subtree_states) {
          int state_updates = update_states.GetUpdateCount(subtree_state_str);
          if (state_updates > 0) {
            search_model_update_states[(int) c.action] += 1;
            search_model_updates[(int) c.action] += state_updates;
          }
        }
      }
    }
    NormalizePolicy(&policy);

    open_spiel::Action action;
    if (depth >= config.temperature_drop) {
      action = root->BestChild().action;
    } else {
      action = open_spiel::SampleAction(policy, *rng).first;
    }

    double root_value = root->total_reward / root->explore_count;

    // Only train on states where full searches were performed
    if (full_search){
      trajectory.states.push_back(Trajectory::State{
        state->ObservationTensor(), player, state->LegalActions(), action, std::move(policy), root_value, state_str, action_sequence, search_states, depth, full_search, (*bots)[player]->IsMCTSSolver(), search_visits, search_priors, search_action_values, search_model_update_states, search_model_updates, search_terminal_states, search_nonterminal_states});
    }

    state->ApplyAction(action);
    action_sequence.push_back(action);
    depth += 1;

    if (state->IsTerminal()) {
      trajectory.returns = state->Returns();
      break;
    } else if (std::abs(root_value) > cutoff_value) {
      trajectory.returns.resize(2);
      trajectory.returns[player] = root_value;
      trajectory.returns[1 - player] = -root_value;
      break;
    }
  }

  return trajectory;
}

// Plays a self-play game or an evaluation match
Trajectory PlayGame(Logger* logger, int game_num, const AlphaZeroConfig& config, const open_spiel::Game& game,
                    std::vector<std::unique_ptr<MCTSBot>>* bots, std::shared_ptr<VPNetEvaluator> vp_eval,
                    std::mt19937* rng, double temperature, int temperature_drop, double cutoff_value,
                    std::string caller, bool use_search_states, bool use_forced_playouts, Archive& archive,
                    UpdateStates& update_states, bool verbose = false) {
  
  std::shared_ptr<open_spiel::State> state = game.NewInitialState();
  std::vector<std::string> history;
  Trajectory trajectory;

  std::vector<open_spiel::Action> action_sequence;
  int depth = 0;

  // Sample random number to determine whether Go-Exploit begins self-play trajectory from sampled state of interest
  absl::uniform_real_distribution<double> dist(0.0, 1.0);
  double go_exploit_rand_num = dist(*rng);

  if (config.katago_init && caller == "Training_Actor") {
    // Mean of Exponential Distribution = 1/lambda
    double mean = 0.04 * config.board_width * config.board_length;
    double lambda = 1.0 / mean;

    std::exponential_distribution<double> distribution(lambda);
    double exp_num = distribution(*rng);
    int num_moves = (int) std::floor(exp_num);

    KataGoInitData katago_init_data = KataGoInitializeGame(game, num_moves, vp_eval, rng);
    state = katago_init_data.state;
    depth = katago_init_data.depth;
    action_sequence = katago_init_data.action_sequence;
  }
  else if (caller == "Training_Actor" && go_exploit_rand_num >= config.start_state_prob) {
    ArchiveEntry entry = archive.Sample();
    // Take Archive State's Action Sequence To Load Archive State
    for (open_spiel::Action seq_action : entry.action_sequence) {
      state->ApplyAction(seq_action);
    }

    depth = entry.depth;
    action_sequence = entry.action_sequence;
  }

  while (true) {
    std::string state_str = state->ToString();
    state_str.erase(std::remove(state_str.begin(), state_str.end(), '\n'), state_str.end());

    open_spiel::Player player = state->CurrentPlayer();
    std::unordered_map<std::string, SearchArchiveEntry> search_states;

    int search_sims;
    bool full_search;
    if (caller == "Evaluator") {
      search_sims = (*bots)[player]->GetMaxSimulations();
      full_search = false;
    }
    else if (config.playout_cap && caller == "Training_Actor") {
      // Playout Cap Randomization
      double cap_rand_num = dist(*rng);
      if (cap_rand_num > config.full_search_prob) {
        search_sims = config.fast_search;
        full_search = false;
      }
      else {
        search_sims = config.max_simulations;
        full_search = true;
      }
    }
    else {
      search_sims = config.max_simulations;
      full_search = true;
    }

    std::unique_ptr<SearchNode> root = (*bots)[player]->MCTSearch(*state, action_sequence, search_states, use_search_states, depth, search_sims, full_search, use_forced_playouts);

    open_spiel::ActionsAndProbs policy;
    policy.reserve(root->children.size());
    std::unordered_map<int, int> search_visits;
    std::unordered_map<int, double> search_priors;
    std::unordered_map<int, double> search_action_values;
    std::unordered_map<int, int> search_model_update_states;
    std::unordered_map<int, int> search_model_updates;
    std::unordered_map<int, int> search_terminal_states;
    std::unordered_map<int, int> search_nonterminal_states;
    for (const SearchNode& c : root->children) {
      policy.emplace_back(c.action, std::pow(c.explore_count, 1.0 / temperature));

      if (config.write_search_stats) {
        // Update Search Stats
        search_visits[(int) c.action] = c.explore_count;
        search_priors[(int) c.action] = c.prior;
        search_action_values[(int) c.action] = c.total_reward / c.explore_count;
        search_model_update_states[(int) c.action] = 0;
        search_model_updates[(int) c.action] = 0;
        search_terminal_states[(int) c.action] = c.terminal_states;
        search_nonterminal_states[(int) c.action] = c.nonterminal_states;

        for (std::string subtree_state_str : c.subtree_states) {
          int state_updates = update_states.GetUpdateCount(subtree_state_str);
          if (state_updates > 0) {
            search_model_update_states[(int) c.action] += 1;
            search_model_updates[(int) c.action] += state_updates;
          }
        }
      }
    }

    NormalizePolicy(&policy);

    open_spiel::Action action;
    if (history.size() >= temperature_drop) {
      action = root->BestChild().action;
    } else {
      action = open_spiel::SampleAction(policy, *rng).first;
    }

    double root_value = root->total_reward / root->explore_count;
    trajectory.states.push_back(Trajectory::State{
        state->ObservationTensor(), player, state->LegalActions(), action, std::move(policy), root_value, state_str, action_sequence, search_states, depth, full_search, (*bots)[player]->IsMCTSSolver(), search_visits, search_priors, search_action_values, search_model_update_states, search_model_updates, search_terminal_states, search_nonterminal_states});
    std::string action_str = state->ActionToString(player, action);
    history.push_back(action_str);
    state->ApplyAction(action);
    action_sequence.push_back(action);
    depth += 1;

    if (verbose) {
      logger->Print("Player: %d, action: %s", player, action_str);
    }
    if (state->IsTerminal()) {
      trajectory.returns = state->Returns();
      break;
    } else if (std::abs(root_value) > cutoff_value) {
      trajectory.returns.resize(2);
      trajectory.returns[player] = root_value;
      trajectory.returns[1 - player] = -root_value;
      break;
    }
  }

  logger->Print("Game %d: Returns: %s; Actions: %s", game_num,
                absl::StrJoin(trajectory.returns, " "),
                absl::StrJoin(history, " "));
  return trajectory;
}

std::unique_ptr<MCTSBot> InitAZBot(const AlphaZeroConfig& config,
                                   const open_spiel::Game& game,
                                   std::shared_ptr<Evaluator> evaluator,
                                   bool evaluation, double uct_c,
                                   int seed) {
  return std::make_unique<MCTSBot>(
      game, std::move(evaluator), uct_c, config.max_simulations,
      /*max_memory_mb=*/10,
      /*solve=*/false,
      /*seed=*/seed,
      /*verbose=*/false, ChildSelectionPolicy::PUCT,
      evaluation ? 0 : config.policy_alpha,
      evaluation ? 0 : config.policy_epsilon);
}

// A training actor thread runner that generates self-play trajectories for the policy-value network to train upon
void training_actor(const open_spiel::Game& game, const AlphaZeroConfig& config, int num,
           ThreadedQueue<Trajectory>* trajectory_queue,
           std::shared_ptr<VPNetEvaluator> vp_eval, StopToken* stop,
           Archive& archive, UpdateStates& update_states) {
  std::unique_ptr<Logger> logger;
  if (num < 20) {  // Limit the number of open files.
    logger.reset(new FileLogger(config.path, absl::StrCat("actor-", num)));
  } else {
    logger.reset(new NoopLogger());
  }
  unsigned seed1 = std::chrono::system_clock::now().time_since_epoch().count();
  std::mt19937 rng(seed1);
  absl::uniform_real_distribution<double> dist(0.0, 1.0);
  std::vector<std::unique_ptr<MCTSBot>> bots;
  bots.reserve(2);
  for (int player = 0; player < 2; player++) {
    int bot_seed = (int) std::chrono::system_clock::now().time_since_epoch().count();
    bots.push_back(InitAZBot(config, game, vp_eval, false, config.uct_c, bot_seed));
  }
  for (int game_num = 1; !stop->StopRequested(); ++game_num) {
    double cutoff =
        (dist(rng) < config.cutoff_probability ? config.cutoff_value
                                               : game.MaxUtility() + 1);

    // Play Regular Training Trajectory
    Trajectory original_trajectory = PlayGame(logger.get(), game_num, config, game, &bots, vp_eval, &rng, config.temperature, config.temperature_drop, cutoff, "Training_Actor", false, config.use_forced_playouts, archive, update_states);

    // KataGo Early Branch
    if (config.game_branching) {
      double rand_num = dist(rng);

      // Branch from early game position in 5% of games
      if (rand_num < 0.05) {
        int num_moves = -1;

        while (num_moves < 0 || num_moves >= original_trajectory.states.size()) {
          double mean = 0.025 * config.board_width * config.board_length;
          double lambda = 1.0 / mean;

          std::exponential_distribution<double> distribution(lambda);
          num_moves = (int) std::floor(distribution(rng));
        }

        std::vector<open_spiel::Action> branched_action_sequence = original_trajectory.states[num_moves].action_sequence;

        // Restore branched state
        std::shared_ptr<open_spiel::State> state_clone = game.NewInitialState();
        for (open_spiel::Action seq_action : branched_action_sequence) {
          state_clone->ApplyAction(seq_action);
        }

        int branched_depth = original_trajectory.states[num_moves].depth;

        // Sample 3-10 legal moves uniformly at random and evaluate
        std::uniform_int_distribution<int> uni(3, 10);
        int num_actions = (int) uni(rng);

        std::vector<open_spiel::Action> legal_actions = state_clone->LegalActions();
        std::vector<open_spiel::Action> sampled_actions;
        std::sample(legal_actions.begin(), legal_actions.end(), std::back_inserter(sampled_actions), num_actions, rng);

        open_spiel::Action best_action = -1;
        double best_value = -std::numeric_limits<double>::infinity();
        std::shared_ptr<open_spiel::State> best_branched_state;
        for (open_spiel::Action curr_action : sampled_actions) {
          std::shared_ptr<open_spiel::State> branched_state = std::move(state_clone->Clone());
          branched_state->ApplyAction(curr_action);
          if (!branched_state->IsTerminal()) {
            open_spiel::Player player = branched_state->CurrentPlayer();
            double state_value = vp_eval->Evaluate(*branched_state)[player];

            if (state_value > best_value) {
              best_value = state_value;
              best_action = curr_action;
              best_branched_state = branched_state;
            }
          }
        }

        if (best_action != -1) {
          branched_action_sequence.push_back(best_action);

          Trajectory branch_trajectory = KataGoBranch(config, best_branched_state, branched_action_sequence, branched_depth + 1, &bots, &rng, cutoff, update_states);
          // Returned trajectory only contains states where full searches were performed
          if (branch_trajectory.states.size() > 0) {
            trajectory_queue->Push(branch_trajectory, absl::Seconds(10));
          }
        }
      }
    }

    // KataGo Recursive Branch
    if (config.position_branching) {
      // In 2.5% of positions, the game is recursively branched
      for (Trajectory::State& state : original_trajectory.states) {
        double rand_num = dist(rng);

        if (rand_num < 0.025) {
          // Restore branched state
          std::shared_ptr<open_spiel::State> recursive_state_clone = game.NewInitialState();
          for (open_spiel::Action seq_action : state.action_sequence) {
            recursive_state_clone->ApplyAction(seq_action);
          }

          KataGoRecursiveBranch(config, recursive_state_clone, state.action_sequence, state.depth, vp_eval, &bots, &rng, state.action, trajectory_queue, update_states);
        }
      }
    }

    // Create trajectory containing only states where full searches were performed.
    // Policy-value network only trains on states where full searches were performed.
    Trajectory full_search_trajectory;
    for (Trajectory::State& state : original_trajectory.states) {
      if (state.full_search) {
        full_search_trajectory.states.push_back(Trajectory::State{
          state.observation, state.current_player, state.legal_actions, state.action, std::move(state.policy), state.value, state.state_str, state.action_sequence, std::move(state.search_states), state.depth, state.full_search, state.is_mcts_solver, std::move(state.search_visits), std::move(state.search_priors), std::move(state.search_action_values), std::move(state.search_model_update_states), std::move(state.search_model_updates), std::move(state.search_terminal_states), std::move(state.search_nonterminal_states)});
      }
    }
    full_search_trajectory.returns = original_trajectory.returns;

    if (full_search_trajectory.states.size() > 0) {
      trajectory_queue->Push(full_search_trajectory, absl::Seconds(10));
    }
  }
  logger->Print("Got a quit.");
}

// A thread that produces trajectories always beginning from the initial state of the game. Adds search states to the archive.
void archive_actor(const open_spiel::Game& game, const AlphaZeroConfig& config, int num,
           std::shared_ptr<VPNetEvaluator> vp_eval, StopToken* stop,
           Archive& archive, UpdateStates& update_states) {
  FileLogger logger(config.path, absl::StrCat("archive_actor-", num));
  unsigned seed1 = std::chrono::system_clock::now().time_since_epoch().count();
  std::mt19937 rng(seed1);
  absl::uniform_real_distribution<double> dist(0.0, 1.0);
  std::vector<std::unique_ptr<MCTSBot>> bots;
  bots.reserve(2);
  for (int player = 0; player < 2; player++) {
    int bot_seed = (int) std::chrono::system_clock::now().time_since_epoch().count();
    bots.push_back(InitAZBot(config, game, vp_eval, false, config.uct_c, bot_seed));
  }
  for (int game_num = 1; !stop->StopRequested(); ++game_num) {
    double cutoff =
        (dist(rng) < config.cutoff_probability ? config.cutoff_value
                                               : game.MaxUtility() + 1);
    // Start State Prob set to 1.0 so that the trajectories from which we get the search stats always begin from the start state to form a fair comparison between Go-Exploit and AlphaZero
    Trajectory trajectory = PlayGame(&logger, game_num, config, game, &bots, vp_eval, &rng, config.temperature, config.temperature_drop, cutoff, "Archive_Actor", config.use_search_states, config.use_forced_playouts, archive, update_states);

    // If Go-Exploit Search States, add search states to archive
    if (config.use_search_states) {
      std::vector<ArchiveEntry> trajectory_archive_entries;
      for (Trajectory::State& state : trajectory.states) {
        for( const auto& entry : state.search_states ) {
          std::string search_state_str = entry.first;
          SearchArchiveEntry search_archive_entry = entry.second;

          ArchiveEntry search_state_entry = {search_archive_entry.action_sequence, search_archive_entry.depth};
          trajectory_archive_entries.push_back(search_state_entry);
        }
      }
      // Update Archive. Transfer archive entries from step_archive to archive
      archive.Update(trajectory_archive_entries);
    }
  }
}

// Processes the evaluation match results against MCTS-Solver
class EvalResults {
 public:
  explicit EvalResults(int count, int evaluation_window) {
    evaluation_window_ = evaluation_window;
    step_ = 1;
    matches_played_.reserve(count);
    results_sum_.reserve(count);
    results_.reserve(count);
    for (int i = 0; i < count; ++i) {
      results_.emplace_back(evaluation_window);
      matches_played_.emplace_back(0);
      results_sum_.emplace_back(0);
    }
  }

  // How many evals per difficulty.
  int EvalCount() {
    absl::MutexLock lock(&m_);
    return eval_num_ / results_.size();
  }

  // Which eval to do next: difficulty, player0.
  std::pair<int, bool> Next() {
    absl::MutexLock lock(&m_);
    int next = eval_num_ % (results_.size() * 2);
    eval_num_ += 1;
    return {next / 2, next % 2};
  }

  void Add(int i, double value, double game_time, double game_length) {
    absl::MutexLock lock(&m_);
    results_[i].Add(value);
    if (step_ > matches_played_[i].size()) {
      for (int j = matches_played_[i].size(); j < step_; ++j) {
        matches_played_[i].push_back(0);
        results_sum_[i].push_back(0);
      }
    }
    matches_played_[i][step_ - 1] += 1;
    results_sum_[i][step_ - 1] += value;
  }

  std::vector<double> AvgResults() {
    absl::MutexLock lock(&m_);
    std::vector<double> out;
    out.reserve(results_.size());
    for (const auto& result : results_) {
      out.push_back(result.Empty() ? 0
                                   : (absl::c_accumulate(result.Data(), 0.0) /
                                      result.Size()));
    }
    return out;
  }

  std::vector<std::vector<double>> StepAvgResults(const std::string &path) {
    absl::MutexLock lock(&m_);
    // std::cout << "Step: " << std::to_string(step_) << " Calculate Match Weighted Eval Results" << std::endl;

    // Ensure size of matches_played_[i] and results_sum_[i] = step_ (If no eval matches finished for a specific difficulty in the given learning step, the size of the vectors won't match step_. Need to address that here)
    for (int i = 0; i < results_.size(); ++i) {
      if (step_ > matches_played_[i].size()) {
        for (int j = matches_played_[i].size(); j < step_; ++j) {
          matches_played_[i].push_back(0);
          results_sum_[i].push_back(0);
        }
      }
      // std::cout << "Difficulty " << std::to_string(i) << " matches_played_[i].size(): " << std::to_string(matches_played_[i].size()) << std::endl;
    }

    // Output Vectors
    std::vector<std::vector<double>> out;

    std::vector<double> step_weighted_performance;
    step_weighted_performance.reserve(results_.size());

    std::vector<double> match_weighted_performance;
    match_weighted_performance.reserve(results_.size());

    // Compute Avg Performance Over Previous "Evaluation Window" Learning Steps
    int min_step = std::max((int) (step_ - evaluation_window_), (int) 0);
    // std::cout << "Min Step: " << std::to_string(min_step) << std::endl;

    for (int i = 0; i < results_.size(); ++i){
      if (matches_played_[i].size() > 0) {
        double steps = 0;
        double avg_outcome_sum = 0;
        double outcome_sum = 0;
        double total_eval_matches = 0;
        for (int j = min_step; j < step_; ++j) {
          if (matches_played_[i][j] > 0) {
            steps += 1;
            total_eval_matches += matches_played_[i][j];
            avg_outcome_sum += (results_sum_[i][j] / matches_played_[i][j]);
            outcome_sum += results_sum_[i][j];
          }
        }
        if (steps > 0) {
          step_weighted_performance.push_back(avg_outcome_sum / steps);
          if (std::abs(avg_outcome_sum / steps) > 1.0) {
            std::cout << "Step Weighted Value Greater Than 1.0" << std::endl;
            std::cout << "avg_outcome_sum: " << std::to_string(avg_outcome_sum) << std::endl;
            std::cout << "steps: " << std::to_string(steps) << std::endl;
          }
        }
        else {
          step_weighted_performance.push_back(-1.0);
        }
        if (total_eval_matches > 0) {
          match_weighted_performance.push_back(outcome_sum / total_eval_matches);
          if (std::abs(outcome_sum / total_eval_matches) > 1.0) {
            std::cout << "Match Weighted Value Greater Than 1.0" << std::endl;
            std::cout << "outcome_sum: " << std::to_string(outcome_sum) << std::endl;
            std::cout << "total_eval_matches: " << std::to_string(total_eval_matches) << std::endl;
          }
        }
        else {
          match_weighted_performance.push_back(-1.0);
        }
      }
      else {
        std::cout << "Difficulty " << std::to_string(i) << " Step " << std::to_string(step_) << " No Eval Matches Yet" << std::endl;
        step_weighted_performance.push_back(-1.0);
        match_weighted_performance.push_back(-1.0);
      }
    }

    out.push_back(step_weighted_performance);
    out.push_back(match_weighted_performance);

    step_ += 1;

    return out;
  }

  void WriteResults(const std::string &path) {
    absl::MutexLock lock(&m_);
    std::ofstream eval_results_file;
    eval_results_file.open(path + "/Eval_Results.csv", std::ios::out);
    eval_results_file << std::to_string(eval_num_) << std::endl;
    for (int i = 0; i < results_.size(); ++i){
      const std::vector<double>& level_results = results_[i].Data();
      for (double outcome : level_results) {
        eval_results_file << std::to_string(outcome) << ",";
      }
      eval_results_file << std::endl;
      int64_t total_added = results_[i].TotalAdded();
      eval_results_file << std::to_string(total_added) << std::endl;
    }
    eval_results_file.close();
  }

  int GetLearningStep() {
    absl::MutexLock lock(&m_);
    return step_;
  }

 private:
  std::vector<CircularBuffer<double>> results_;
  std::vector<std::vector<double>> matches_played_;
  std::vector<std::vector<double>> results_sum_;
  double evaluation_window_;
  int step_ = 1;
  int eval_num_ = 0;
  absl::Mutex m_;
};

// A thread that plays vs MCTS-Solver
void evaluator(const open_spiel::Game& game, const AlphaZeroConfig& config,
               int num, EvalResults* results,
               std::shared_ptr<VPNetEvaluator> vp_eval, StopToken* stop,
               Archive& archive, UpdateStates& update_states) {
  FileLogger logger(config.path, absl::StrCat("evaluator-", num));
  unsigned seed1 = std::chrono::system_clock::now().time_since_epoch().count();
  std::mt19937 rng(seed1);
  int rollout_seed = (int) std::chrono::system_clock::now().time_since_epoch().count();
  auto rand_evaluator = std::make_shared<RandomRolloutEvaluator>(1, rollout_seed);

  for (int game_num = 1; !stop->StopRequested(); ++game_num) {
    auto [difficulty, first] = results->Next();
    int learning_step = results->GetLearningStep();
    int az_player = first ? 0 : 1;
    int rand_max_simulations =
        config.max_simulations * std::pow(10, difficulty / 2.0);
    std::vector<std::unique_ptr<MCTSBot>> bots;
    bots.reserve(2);
    int bot1_seed = (int) std::chrono::system_clock::now().time_since_epoch().count();
    bots.push_back(InitAZBot(config, game, vp_eval, true, config.uct_c, bot1_seed));
    int bot2_seed = (int) std::chrono::system_clock::now().time_since_epoch().count();
    bots.push_back(std::make_unique<MCTSBot>(
        game, rand_evaluator, config.eval_uct_c, rand_max_simulations,
        /*max_memory_mb=*/1000,
        /*solve=*/true,
        /*seed=*/bot2_seed,
        /*verbose=*/false, ChildSelectionPolicy::UCT));
    if (az_player == 1) {
      std::swap(bots[0], bots[1]);
    }

    logger.Print("Running MCTS with %d simulations", rand_max_simulations);
    absl::Time eval_game_start = absl::Now();
    Trajectory trajectory = PlayGame(
        &logger, game_num, config, game, &bots, vp_eval, &rng, /*temperature=*/1,
        /*temperature_drop=*/0, /*cutoff_value=*/game.MaxUtility() + 1, "Evaluator", false, false, archive, update_states);
    absl::Time eval_game_end = absl::Now();
    double eval_game_time = absl::ToDoubleSeconds(eval_game_end - eval_game_start);
    // std::cout << "Eval Thread " << num << " Game " << game_num << " Difficulty " << difficulty << " Game Time: " << std::to_string(eval_game_time) << " Game Length: " << std::to_string(trajectory.states.size()) << " Game Result: " << std::to_string(trajectory.returns[az_player]) << std::endl;

    // Write Eval Search Stats
    if (config.write_search_stats) {
      absl::MutexLock lock(&eval_mutex_);
      for (Trajectory::State& state : trajectory.states) {
        if (!state.is_mcts_solver) {
          std::ofstream eval_search_stats_file;
          eval_search_stats_file.open(config.path + "/Eval_Search_Stats_Step_" + std::to_string(learning_step) + "_Level_" + std::to_string(difficulty) + ".csv", std::ios::out | std::ios::app);
          eval_search_stats_file << state.state_str << "," << std::to_string(state.action) << "," << std::to_string(state.depth) << std::endl;
          for( auto& entry : state.search_visits ) {
            int action = entry.first;
            eval_search_stats_file << std::to_string(action) << "," << std::to_string(state.search_visits[action]) << "," << std::to_string(state.search_priors[action]) << "," << std::to_string(state.search_action_values[action]) << "," << std::to_string(state.search_model_update_states[action]) << "," << std::to_string(state.search_model_updates[action]) << "," << std::to_string(state.search_terminal_states[action]) << "," << std::to_string(state.search_nonterminal_states[action]) << std::endl;
          }
          eval_search_stats_file.close();
        }
      }
    }

    results->Add(difficulty, trajectory.returns[az_player], eval_game_time, (double) trajectory.states.size());
    logger.Print("Game %d: AZ: %5.2f, MCTS: %5.2f, MCTS-sims: %d, length: %d",
                 game_num, trajectory.returns[az_player],
                 trajectory.returns[1 - az_player], rand_max_simulations,
                 trajectory.states.size());
  }
  logger.Print("Got a quit.");
}

// Processes the self-play trajectories produced by training actors and updates the policy-value network
void learner(const open_spiel::Game& game, const AlphaZeroConfig& config,
             DeviceManager* device_manager,
             std::shared_ptr<VPNetEvaluator> eval,
             ThreadedQueue<Trajectory>* trajectory_queue,
             EvalResults* eval_results, StopToken* stop,
             const StartInfo& start_info,
             Archive& archive, UpdateStates& update_states) {
  absl::Time time_start = absl::Now();
  FileLogger logger(config.path, "learner", "a");
  DataLoggerJsonLines data_logger(
      config.path, "learner", true, "a", start_info.start_time);
  unsigned seed = std::chrono::system_clock::now().time_since_epoch().count();
  std::mt19937 rng(seed);

  int device_id = 0;  // Do not change, the first device is the learner.
  logger.Print("Running the learner on device %d: %s", device_id,
               device_manager->Get(0, device_id)->Device());

  SerializableCircularBuffer<VPNetModel::TrainInputs> replay_buffer(
      config.replay_buffer_size);
  if (start_info.start_step > 1) {
    std::cout << "Loading Saved Replay Buffer" << std::endl;
    replay_buffer.LoadBuffer(config.path + "/replay_buffer.data");
    std::cout << "Finished Loading Saved Replay Buffer" << std::endl;
  }
  int learn_rate = config.replay_buffer_size / config.replay_buffer_reuse;
  int64_t total_trajectories = start_info.total_trajectories;

  std::vector<ArchiveEntry> step_archive; // Holds archive entries created in current learning step. Transferred over to real archive at the end of the step. Only used by Go-Exploit Visited States (no search states)

  const int stage_count = 7;
  std::vector<open_spiel::BasicStats> value_accuracies(stage_count);
  std::vector<open_spiel::BasicStats> value_predictions(stage_count);
  open_spiel::BasicStats game_lengths;
  open_spiel::HistogramNumbered game_lengths_hist(game.MaxGameLength() + 1);

  open_spiel::HistogramNamed outcomes({"Player1", "Player2", "Draw"});
  // Actor threads have likely been contributing for a while, so put `last` in
  // the past to avoid a giant spike on the first step.
  absl::Time last = absl::Now() - absl::Seconds(60);
  for (int step = start_info.start_step; !stop->StopRequested() && (config.max_steps == 0 || step <= config.max_steps); ++step) {
    absl::Time step_start = absl::Now();
    std::cout << "Step: " << std::to_string(step) << std::endl;

    outcomes.Reset();
    game_lengths.Reset();
    game_lengths_hist.Reset();

    step_archive.clear();

    for (auto& value_accuracy : value_accuracies) {
      value_accuracy.Reset();
    }
    for (auto& value_prediction : value_predictions) {
      value_prediction.Reset();
    }

    // Collect trajectories
    int queue_size = trajectory_queue->Size();
    int num_states = 0;
    int num_trajectories = 0;
    while (!stop->StopRequested() && num_states < learn_rate) {
      absl::optional<Trajectory> trajectory = trajectory_queue->Pop();
      if (trajectory) {
        num_trajectories += 1;
        total_trajectories += 1;
        game_lengths.Add(trajectory->states.size());
        game_lengths_hist.Add(trajectory->states.size());

        double p1_outcome = trajectory->returns[0];
        outcomes.Add(p1_outcome > 0 ? 0 : (p1_outcome < 0 ? 1 : 2));

        for (Trajectory::State& state : trajectory->states) {
          replay_buffer.Add(VPNetModel::TrainInputs{state.state_str, state.legal_actions,
                                                    state.observation,
                                                    state.policy, p1_outcome});
          num_states += 1;

          // If Go-Exploit Visited States, update Step Archive
          if (config.start_state_prob < 1.0 && !config.use_search_states) {
            ArchiveEntry state_entry = {state.action_sequence, state.depth};
            step_archive.push_back(state_entry);
          }

          if (config.write_search_stats) {
            /*
            Search Stats File organized as follows:
            - Self-Play State, Action played, Depth
            - Action 0: visits, prior, Q(s,a), states in subtree used to update model, total number of times states in subtree have been used to update model, number of terminal states in subtree, number of nonterminal states in subtree
            ...
            - Action k: visits, prior, Q(s,a), states in subtree used to update model, total number of times states in subtree have been used to update model, number of terminal states in subtree, number of nonterminal states in subtree
            */
            std::ofstream self_play_search_stats_file;
            self_play_search_stats_file.open(config.path + "/Self_Play_Search_Stats_Step_" + std::to_string(step) + ".csv", std::ios::out | std::ios::app);
            self_play_search_stats_file << state.state_str << "," << std::to_string(state.action) << "," << std::to_string(state.depth) << std::endl;
            for( auto& entry : state.search_visits ) {
              int action = entry.first;
              self_play_search_stats_file << std::to_string(action) << "," << std::to_string(state.search_visits[action]) << "," << std::to_string(state.search_priors[action]) << "," << std::to_string(state.search_action_values[action]) << "," << std::to_string(state.search_model_update_states[action]) << "," << std::to_string(state.search_model_updates[action]) << "," << std::to_string(state.search_terminal_states[action]) << "," << std::to_string(state.search_nonterminal_states[action]) << std::endl;
            }
            self_play_search_stats_file.close();
          }
        }

        for (int stage = 0; stage < stage_count; ++stage) {
          // Scale for the length of the game
          int index = (trajectory->states.size() - 1) *
                      static_cast<double>(stage) / (stage_count - 1);
          const Trajectory::State& s = trajectory->states[index];
          value_accuracies[stage].Add(
              (s.value >= 0) == (trajectory->returns[s.current_player] >= 0));
          value_predictions[stage].Add(abs(s.value));
        }
      }
    }

    absl::Time now = absl::Now();
    double seconds = absl::ToDoubleSeconds(now - last);

    logger.Print("Step: %d", step);
    logger.Print(
        "Collected %5d states from %3d games, %.1f states/s; "
        "%.1f states/(s*actor), game length: %.1f",
        num_states, num_trajectories, num_states / seconds,
        num_states / (config.training_actors * seconds),
        static_cast<double>(num_states) / num_trajectories);
    logger.Print("Queue size: %d. Buffer size: %d. States seen: %d", queue_size,
                 replay_buffer.Size(), replay_buffer.TotalAdded());

    if (stop->StopRequested()) {
      break;
    }

    last = now;

    absl::Time save_buffer_start = absl::Now();
    replay_buffer.SaveBuffer(config.path + "/replay_buffer.data");
    absl::Time save_buffer_end = absl::Now();
    double save_buffer_time = absl::ToDoubleSeconds(save_buffer_end - save_buffer_start);
    std::cout << "Save Replay Buffer Time: " << std::to_string(save_buffer_time) << std::endl;

    // Save Eval Results
    eval_results->WriteResults(config.path);

    absl::Time model_update_start = absl::Now();
    VPNetModel::LossInfo losses;
    std::string checkpoint_path = "";
    {  // Extra scope to return the device for use for inference asap.
      // std::cout << "Get Device 0 For Model Learning" << std::endl;
      DeviceManager::DeviceLoan learn_model =
          device_manager->Get(config.train_batch_size, device_id);

      // Let the device manager know that the first device is now
      // off-limits for inference and should only be used for learning
      // (if config.explicit_learning == true).
      device_manager->SetLearning(config.explicit_learning);
      // std::cout << "Explicit Learning Set To True" << std::endl;

      // std::cout << "Begin Learning" << std::endl;
      // Learn from them.
      for (int i = 0; i < learn_rate / config.train_batch_size; i++) {
        std::vector<VPNetModel::TrainInputs> learn_inputs = replay_buffer.Sample(&rng, config.train_batch_size);
        if (config.write_search_stats) {
          update_states.Update(learn_inputs);
        }
        losses += learn_model->Learn(learn_inputs);
      }

      if (step > 0 && step % 50 == 0 && config.write_search_stats) {
        update_states.Write(config.path, step);
      }

      // Always save a checkpoint, either for keeping or for loading the weights
      // to the other sessions. It only allows numbers, so use -1 as "latest".
      // std::cout << "Save Updated Model Checkpoint" << std::endl;
      checkpoint_path = learn_model->SaveCheckpoint(VPNetModel::kMostRecentCheckpointStep);
      if (step % config.checkpoint_freq == 0) {
        learn_model->SaveCheckpoint(step);
      }

      // The device manager can now once again use the first device for
      // inference (if it could not before).
      device_manager->SetLearning(false);
      // std::cout << "Explicit Learning Set To False" << std::endl;
    }

    // std::cout << "Begin Updating Other Models" << std::endl;
    if (device_manager->Count() > 0) {
      for (int i = 0; i < device_manager->Count(); ++i) {
        if (i != device_id) {
          device_manager->SetModelUpdating(i, true);
          // std::cout << "Updating Model On Device " << i << std::endl;
          device_manager->Get(0, i)->LoadCheckpoint(checkpoint_path);
          // std::cout << "Finished Updating Model On Device " << i << std::endl;
          device_manager->SetModelUpdating(i, false);
        }
      }
    }
    // std::cout << "Finished Updating Other Models" << std::endl;
    logger.Print("Checkpoint saved: %s", checkpoint_path);

    absl::Time model_update_end = absl::Now();
    double model_update_time = absl::ToDoubleSeconds(model_update_end - model_update_start);
    std::cout << "Model Update Time: " << std::to_string(model_update_time) << std::endl;

    // If Go-Exploit Visited States, Update Archive. Transfer archive entries from step_archive to archive
    if (config.start_state_prob < 1.0 && !config.use_search_states) {
      archive.Update(step_archive);
    }

    std::vector<std::vector<double>> step_eval_results = eval_results->StepAvgResults(config.path);
    std::vector<double> step_weighted_performance = step_eval_results[0];
    std::vector<double> match_weighted_performance = step_eval_results[1];

    DataLogger::Record record = {
        {"step", step},
        {"total_states", replay_buffer.TotalAdded()},
        {"states_per_s", num_states / seconds},
        {"states_per_s_actor", num_states / (config.training_actors * seconds)},
        {"total_trajectories", total_trajectories},
        {"trajectories_per_s", num_trajectories / seconds},
        {"queue_size", queue_size},
        {"game_length", game_lengths.ToJson()},
        {"game_length_hist", game_lengths_hist.ToJson()},
        {"outcomes", outcomes.ToJson()},
        {"value_accuracy",
         json::TransformToArray(value_accuracies,
                                [](auto v) { return v.ToJson(); })},
        {"value_prediction",
         json::TransformToArray(value_predictions,
                                [](auto v) { return v.ToJson(); })},
        {"eval", json::Object({
                     {"count", eval_results->EvalCount()},
                     {"window_results", json::CastToArray(eval_results->AvgResults())},
                     {"step_weighted_results", json::CastToArray(step_weighted_performance)},
                     {"match_weighted_results", json::CastToArray(match_weighted_performance)},
                 })},
        {"batch_size", eval->BatchSizeStats().ToJson()},
        {"batch_size_hist", eval->BatchSizeHistogram().ToJson()},
        {"loss", json::Object({
                     {"policy", losses.Policy()},
                     {"value", losses.Value()},
                     {"l2reg", losses.L2()},
                     {"sum", losses.Total()},
                 })},
    };
    eval->ResetBatchSizeStats();
    logger.Print("Losses: policy: %.4f, value: %.4f, l2: %.4f, sum: %.4f",
                 losses.Policy(), losses.Value(), losses.L2(), losses.Total());

    LRUCacheInfo cache_info = eval->CacheInfo();
    if (cache_info.size > 0) {
      logger.Print(absl::StrFormat(
          "Cache size: %d/%d: %.1f%%, hits: %d, misses: %d, hit rate: %.3f%%",
          cache_info.size, cache_info.max_size, 100.0 * cache_info.Usage(),
          cache_info.hits, cache_info.misses, 100.0 * cache_info.HitRate()));
      eval->ClearCache();
    }
    record.emplace("cache",
                   json::Object({
                       {"size", cache_info.size},
                       {"max_size", cache_info.max_size},
                       {"usage", cache_info.Usage()},
                       {"requests", cache_info.Total()},
                       {"requests_per_s", cache_info.Total() / seconds},
                       {"hits", cache_info.hits},
                       {"misses", cache_info.misses},
                       {"misses_per_s", cache_info.misses / seconds},
                       {"hit_rate", cache_info.HitRate()},
                   }));

    data_logger.Write(record);
    logger.Print("");

    absl::Time step_end = absl::Now();
    double step_time = absl::ToDoubleSeconds(step_end - step_start);
    double elapsed_time = absl::ToDoubleSeconds(step_end - time_start);
    std::cout << "Step Time: " << std::to_string(step_time) << std::endl;
    std::cout << "Elapsed Time: " << std::to_string(elapsed_time) << std::endl;
  }
}

bool AlphaZero(AlphaZeroConfig config, StopToken* stop, bool resuming) {
  std::shared_ptr<const open_spiel::Game> game;
  if (config.game == "go") {
    int board_size = 9;
    game = LoadGame("go", {{"board_size", open_spiel::GameParameter(board_size)}});
  }
  else {
    game = open_spiel::LoadGame(config.game);
  }

  open_spiel::GameType game_type = game->GetType();
  if (game->NumPlayers() != 2)
    open_spiel::SpielFatalError("AlphaZero can only handle 2-player games.");
  if (game_type.reward_model != open_spiel::GameType::RewardModel::kTerminal)
    open_spiel::SpielFatalError("Game must have terminal rewards.");
  if (game_type.dynamics != open_spiel::GameType::Dynamics::kSequential)
    open_spiel::SpielFatalError("Game must have sequential turns.");
  if (game_type.chance_mode != open_spiel::GameType::ChanceMode::kDeterministic)
    open_spiel::SpielFatalError("Game must be deterministic.");

  file::Mkdirs(config.path);
  if (!file::IsDirectory(config.path)) {
    std::cerr << config.path << " is not a directory." << std::endl;
    return false;
  }

  std::cout << "Logging directory: " << config.path << std::endl;

  if (config.graph_def.empty()) {
    config.graph_def = "vpnet.pb";
    std::string model_path = absl::StrCat(config.path, "/", config.graph_def);
    if (file::Exists(model_path)) {
      std::cout << "Overwriting existing model: " << model_path << std::endl;
    } else {
      std::cout << "Creating model: " << model_path << std::endl;
    }
    SPIEL_CHECK_TRUE(CreateGraphDef(
        *game, config.learning_rate, config.weight_decay, config.path,
        config.graph_def, config.nn_model, config.nn_width, config.nn_depth));
  } else {
    std::string model_path = absl::StrCat(config.path, "/", config.graph_def);
    if (file::Exists(model_path)) {
      std::cout << "Using existing model: " << model_path << std::endl;
    } else {
      std::cout << "Model not found: " << model_path << std::endl;
    }
  }

  std::cout << "Playing game: " << config.game << std::endl;

  config.inference_batch_size = std::max(
      1,
      std::min(config.inference_batch_size, config.training_actors + config.archive_actors + config.evaluators));

  config.inference_threads =
      std::max(1, std::min(config.inference_threads,
                           (1 + config.training_actors + config.archive_actors + config.evaluators) / 2));

  {
    file::File fd(config.path + "/config.json", "w");
    fd.Write(json::ToString(config.ToJson(), true) + "\n");
  }

  StartInfo start_info = {/*start_time=*/absl::Now(),
                          /*start_step=*/1,
                          /*model_checkpoint_step=*/0,
                          /*total_trajectories=*/0};
  if (resuming) {
    std::cout << "Load Start Info" << std::endl;
    start_info = StartInfoFromLearnerJson(config.path);
    std::cout << "Loaded Start Step: " << std::to_string(start_info.start_step) << std::endl;
    std::cout << "Loaded Model Checkpoint: " << std::to_string(start_info.model_checkpoint_step) << std::endl;
  }

  DeviceManager device_manager;
  for (const absl::string_view& device : absl::StrSplit(config.devices, ',')) {
    device_manager.AddDevice(
        VPNetModel(*game, config.path, config.graph_def, std::string(device)));
  }

  if (device_manager.Count() == 0) {
    std::cerr << "No devices specified?" << std::endl;
    return false;
  }

  // The explicit_learning option should only be used when multiple
  // devices are available (so that inference can continue while
  // also undergoing learning).
  if (device_manager.Count() <= 1 && config.explicit_learning) {
    std::cerr << "Explicit learning can only be used with multiple devices."
              << std::endl;
    return false;
  }

  std::cerr << "Loading model from step " << start_info.model_checkpoint_step
            << std::endl;
  {  // Make sure they're all in sync.
    if (!resuming) {
      std::cout << "Save Randomly Initialized Policy-Value Network" << std::endl;
      device_manager.Get(0)->SaveCheckpoint(start_info.model_checkpoint_step);
    }
    for (int i = 0; i < device_manager.Count(); ++i) {
      device_manager.Get(0, i)->LoadCheckpoint(
          start_info.model_checkpoint_step);
    }
  }

  auto eval = std::make_shared<VPNetEvaluator>(
      &device_manager, config.inference_batch_size, config.inference_threads,
      config.inference_cache, (config.training_actors + config.archive_actors + config.evaluators) / 16);

  ThreadedQueue<Trajectory> trajectory_queue(config.replay_buffer_size /
                                             config.replay_buffer_reuse);

  EvalResults eval_results(config.eval_levels, config.evaluation_window);

  // Initialize Archive
  unsigned seed = std::chrono::system_clock::now().time_since_epoch().count();
  Archive archive(config.archive_type, config.max_archive_size, seed, config.path);

  // States used to update the policy-value network
  UpdateStates update_states;

  std::vector<Thread> training_actors;
  training_actors.reserve(config.training_actors);
  for (int i = 0; i < config.training_actors; ++i) {
    training_actors.emplace_back(
        [&, i]() { training_actor(*game, config, i, &trajectory_queue, eval, stop, archive, update_states); });
  }
  std::vector<Thread> archive_actors;
  archive_actors.reserve(config.archive_actors);
  for (int i = 0; i < config.archive_actors; ++i) {
    archive_actors.emplace_back(
        [&, i]() { archive_actor(*game, config, i, eval, stop, archive, update_states); });
  }
  std::vector<Thread> evaluators;
  evaluators.reserve(config.evaluators);
  for (int i = 0; i < config.evaluators; ++i) {
    evaluators.emplace_back(
        [&, i]() { evaluator(*game, config, i, &eval_results, eval, stop, archive, update_states); });
  }
  learner(*game, config, &device_manager, eval, &trajectory_queue,
          &eval_results, stop, start_info, archive, update_states);

  if (!stop->StopRequested()) {
    stop->Stop();
  }

  // Empty the queue so that the actors can exit.
  trajectory_queue.BlockNewValues();
  trajectory_queue.Clear();

  std::cout << "Joining all the threads." << std::endl;
  for (auto& t : training_actors) {
    t.join();
  }
  for (auto& t : archive_actors) {
    t.join();
  }
  for (auto& t : evaluators) {
    t.join();
  }
  std::cout << "Exiting cleanly." << std::endl;
  return true;
}

}  // namespace torch_az
}  // namespace algorithms
}  // namespace open_spiel
