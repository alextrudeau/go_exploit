// Copyright 2019 DeepMind Technologies Ltd. All rights reserved.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

#include "open_spiel/algorithms/alpha_zero_torch/alpha_zero.h"

#include <algorithm>
#include <cstdlib>
#include <iostream>
#include <memory>
#include <random>
#include <string>
#include <utility>
#include <vector>

#include "open_spiel/abseil-cpp/absl/algorithm/container.h"
#include "open_spiel/abseil-cpp/absl/random/uniform_real_distribution.h"
#include "open_spiel/abseil-cpp/absl/strings/str_cat.h"
#include "open_spiel/abseil-cpp/absl/strings/str_join.h"
#include "open_spiel/abseil-cpp/absl/strings/str_split.h"
#include "open_spiel/abseil-cpp/absl/synchronization/mutex.h"
#include "open_spiel/abseil-cpp/absl/time/clock.h"
#include "open_spiel/abseil-cpp/absl/time/time.h"
#include "open_spiel/algorithms/alpha_zero_torch/device_manager.h"
#include "open_spiel/algorithms/alpha_zero_torch/vpevaluator.h"
#include "open_spiel/algorithms/alpha_zero_torch/vpnet.h"
#include "open_spiel/algorithms/mcts.h"
#include "open_spiel/spiel.h"
#include "open_spiel/spiel_utils.h"
#include "open_spiel/utils/circular_buffer.h"
#include "open_spiel/utils/data_logger.h"
#include "open_spiel/utils/file.h"
#include "open_spiel/utils/json.h"
#include "open_spiel/utils/logger.h"
#include "open_spiel/utils/lru_cache.h"
#include "open_spiel/utils/serializable_circular_buffer.h"
#include "open_spiel/utils/stats.h"
#include "open_spiel/utils/thread.h"
#include "open_spiel/utils/threaded_queue.h"

#include <unordered_map>
#include <mutex>
#include <chrono>
#include <fstream>
#include <sstream>

namespace open_spiel {
namespace algorithms {
namespace torch_az {

absl::Mutex eval_mutex_;

// Go-Exploit's archive of "states of interest"
// archive_type: The structure of the archive. If "Expanding", an expanding archive consisting of every state of interest is used.
//     If "Circular", a fixed-size circular archive consisting of the most recently observed/visited states of interest is used.
//     If "Reservoir", a fixed-size archive is maintained with Reservoir Sampling.
// max_size: The capacity of the archive if a fixed-size archive is used.
// seed: The seed for the random number generator used for sampling from the archive.
Archive::Archive(std::string archive_type, int max_size, unsigned seed)
  : archive_type_(archive_type),
    fixed_size_archive_(max_size),
    max_size_(max_size),
    rng_(seed),
    reservoir_index_(max_size) {

  std::cout << "Initializing Archive" << std::endl;

  std::vector<open_spiel::Action> action_sequence;
  
  // Initializing the archive with the initial state of the game
  ArchiveEntry initial_state_entry = {action_sequence, 0};

  // Fixed-Size Circular Archive uses the SerializableCircularBuffer
  // Fixed-Size Archive with Reservoir Sampling represents the archive as a std::vector
  // Expanding Archive also represents the archive as a std::vector
  if (archive_type_ == "Circular") {
    fixed_size_archive_.Add(initial_state_entry);
  }
  else {
    expanding_archive_.push_back(initial_state_entry);
    weights_.push_back(1.0);
    current_size_ = 1;
  }
}

// Samples the initial state of a self-play trajectory from the archive uniformly at random
ArchiveEntry Archive::Sample() {
  absl::MutexLock lock(&archive_mutex_);

  if (archive_type_ == "Circular") {
    std::vector<ArchiveEntry> sampled_entry = fixed_size_archive_.Sample(&rng_, 1);
    return sampled_entry[0];
  }
  else {
    std::discrete_distribution<int> archive_dist(weights_.begin(), weights_.end());
    int state_index = archive_dist(rng_);
    return expanding_archive_[state_index];
  }
}

// Adds the states of interest observed/visited in the most recent learning step to the archive.
// The way states are added/removed from the archive depends upon the archive_type
void Archive::Update(std::vector<ArchiveEntry>& step_archive) {
  absl::MutexLock lock(&archive_mutex_);
  
  for (ArchiveEntry entry : step_archive) {
    if (archive_type_ == "Circular") {
      fixed_size_archive_.Add(entry);
    }
    else if (archive_type_ == "Reservoir") {
      if (current_size_ < max_size_) {
        weights_.push_back(1.0);
        expanding_archive_.push_back(entry);
        current_size_ += 1;
      }
      else {
        // Reservoir Sampling
        std::uniform_int_distribution<int> uni(0, reservoir_index_);
        int random_integer = (int) uni(rng_);

        if (random_integer < max_size_) {
          // Replace existing entry with new entry
          double weight = 1.0;
          weights_[random_integer] = weight;
          expanding_archive_[random_integer] = entry;
        }
        reservoir_index_ += 1;
      }
    }
    // Expanding Archive
    else {
      weights_.push_back(1.0);
      expanding_archive_.push_back(entry);
    }
  }

  if (archive_type_ == "Circular") {
    std::cout << "States in Archive: " << std::to_string(fixed_size_archive_.Size()) << std::endl;
  }
  else {
    std::cout << "States in Archive: " << std::to_string(expanding_archive_.size()) << std::endl;
  }
}

void UpdateStates::Update(std::vector<VPNetModel::TrainInputs> learn_inputs) {
  absl::MutexLock lock(&mutex_);
  for (VPNetModel::TrainInputs train_input : learn_inputs) {
    std::string train_input_str = train_input.state_str;
    if (update_states_.find(train_input_str) == update_states_.end()) {
      update_states_[train_input_str] = 1;
    }
    else{
      update_states_[train_input_str] += 1;
    }
  }
}

int UpdateStates::GetUpdateCount(std::string state_str) {
  absl::MutexLock lock(&mutex_);
  if (update_states_.find(state_str) == update_states_.end()) {
    return 0;
  }
  else {
    return update_states_[state_str];
  }
}

// Write states that have been used to update the policy-value network to a file
void UpdateStates::Write(std::string path, int step) {
  absl::MutexLock lock(&mutex_);
  std::ofstream update_states_file;
  update_states_file.open(path + "/Update_States_Step_" + std::to_string(step) + ".csv", std::ios::out | std::ios::app);
  for( auto& entry : update_states_ ) {
    std::string state_str = entry.first;
    int model_updates = entry.second;

    update_states_file << state_str << "," << std::to_string(model_updates) << std::endl;
  }
  update_states_file.close();
}

struct StartInfo {
  absl::Time start_time;
  int start_step;
  int model_checkpoint_step;
  int64_t total_trajectories;
};

// If resuming from checkpoint, reads starting info from learner.jsonl
StartInfo StartInfoFromLearnerJson(const std::string& path) {
  StartInfo start_info;
  file::File learner_file(path + "/learner.jsonl", "r");
  std::vector<std::string> learner_lines = absl::StrSplit(
      learner_file.ReadContents(), "\n");
  std::string last_learner_line;

  // Get the last non-empty line in learner.jsonl.
  for (int i = learner_lines.size() - 1; i >= 0; i--) {
    if (!learner_lines[i].empty()) {
      last_learner_line = learner_lines[i];
      break;
    }
  }

  json::Object last_learner_json = json::FromString(
      last_learner_line).value().GetObject();

  start_info.start_time = absl::Now() - absl::Seconds(
      last_learner_json["time_rel"].GetDouble());
  start_info.start_step = last_learner_json["step"].GetInt() + 1;
  start_info.model_checkpoint_step = VPNetModel::kMostRecentCheckpointStep;
  start_info.total_trajectories =
      last_learner_json["total_trajectories"].GetInt();

  return start_info;
}

struct Trajectory {
  struct State {
    // Members used for producing training samples
    std::vector<float> observation;
    open_spiel::Player current_player;
    std::vector<open_spiel::Action> legal_actions;
    open_spiel::Action action;
    open_spiel::ActionsAndProbs policy;
    double value;
    // Members used for Go-Exploit
    std::string state_str;
    std::vector<open_spiel::Action> action_sequence;
    std::unordered_map<std::string, SearchArchiveEntry> search_states;
    // Members used for collecting statistics
    int depth;
    bool full_search;
    bool is_mcts_solver;
    std::unordered_map<int, int> search_visits;
    std::unordered_map<int, double> search_priors;
    std::unordered_map<int, double> search_action_values;
    std::unordered_map<int, int> search_model_update_states;
    std::unordered_map<int, int> search_model_updates;
    std::unordered_map<int, int> search_terminal_states;
    std::unordered_map<int, int> search_nonterminal_states;
  };

  std::vector<State> states;
  std::vector<double> returns;
};

// KataGo's trajectory initialization scheme. Activated if config.katago_init=true.
// Returns a KataGoInitData object consisting of the data pertaining to the sampled initial state.
// Learn more in Appendix D: https://arxiv.org/pdf/1902.10565.pdf
// init_moves: The number of actions to sample from the policies output by the policy-value network
KataGoInitData KataGoInitializeGame(const open_spiel::Game& game, int init_moves,
                                    std::shared_ptr<VPNetEvaluator> vp_eval, std::mt19937* rng) {
  std::shared_ptr<open_spiel::State> state = game.NewInitialState();
  std::vector<open_spiel::Action> action_sequence;

  // Sampling actions for init_moves
  std::shared_ptr<open_spiel::State> previous_state;
  for (int i = 0; i < init_moves; ++i) {
    previous_state = state->Clone();
    open_spiel::ActionsAndProbs policy = vp_eval->Prior(*state);
    NormalizePolicy(&policy);
    open_spiel::Action action = open_spiel::SampleAction(policy, *rng).first;
    state->ApplyAction(action);

    // If an action leads to a terminal state, we return the previous state
    if (state->IsTerminal()) {
      std::cout << "KataGo Init State Is Terminal. Returning Previous State" << std::endl;
      KataGoInitData katago_init_data = {previous_state, action_sequence, i};
      return katago_init_data;
    }
    else {
      action_sequence.push_back(action);
    }
  }

  KataGoInitData katago_init_data = {state, action_sequence, init_moves};

  return katago_init_data;
}

// KataGo's position branching scheme. Enabled when config.position_branching=true.
// 2.5% of positions are branched to try an alternative move sampled from the policy output by the policy-value network
// Learn more in Appendix D: https://arxiv.org/pdf/1902.10565.pdf
// banned_action: The action that was taken in the original trajectory. An alternative action will now be taken from the given state.
int KataGoRecursiveBranch(const AlphaZeroConfig& config, std::shared_ptr<open_spiel::State> state, std::vector<open_spiel::Action> action_sequence,
                    int depth, std::shared_ptr<VPNetEvaluator> vp_eval, std::vector<std::unique_ptr<MCTSBot>>* bots,
                    std::mt19937* rng, open_spiel::Action banned_action, ThreadedQueue<Trajectory>* trajectory_queue,
                    UpdateStates& update_states) {
  Trajectory trajectory;

  absl::uniform_real_distribution<double> dist(0.0, 1.0);
  double rand_num = dist(*rng);

  open_spiel::ActionsAndProbs legal_actions = vp_eval->Prior(*state);
  open_spiel::ActionsAndProbs policy;

  // Softmax temperature applied to policy output by the policy-value network
  // Banned action removed from policy
  for (int i = 0; i < legal_actions.size(); i++) {
    if (legal_actions[i].first != banned_action) {
      if (rand_num < 0.7) {
        policy.emplace_back(legal_actions[i].first, std::pow(legal_actions[i].second, 1.0));
      }
      else if (rand_num < 0.95) {
        policy.emplace_back(legal_actions[i].first, std::pow(legal_actions[i].second, 0.5));
      }
      else {
        // Each action has equal probability when temperature equals infinity
        policy.emplace_back(legal_actions[i].first, 1.0);
      }
    }
  }

  if (policy.size() > 0) {
    NormalizePolicy(&policy);
  }

  // Sampling alternative action. Banned action chosen if it's the only legal action
  open_spiel::Action action;
  if (policy.size() > 0) {
    if (rand_num < 0.95) {
      action = open_spiel::SampleAction(policy, *rng).first;
    }
    else {
      std::shuffle(policy.begin(), policy.end(), *rng);
      action = policy[0].first;
    }
  }
  else {
    action = banned_action;
  }

  state->ApplyAction(action);
  depth += 1;
  action_sequence.push_back(action);

  std::string state_str = state->ToString();
  state_str.erase(std::remove(state_str.begin(), state_str.end(), '\n'), state_str.end());

  // Only want to train on and recursively continue from non-terminal states
  if (state->IsTerminal()){
    return 0;
  }
  else {
    // Perform full search to create policy and value target
    open_spiel::Player player = state->CurrentPlayer();
    std::unordered_map<std::string, SearchArchiveEntry> search_states;
    std::unique_ptr<SearchNode> root = (*bots)[player]->MCTSearch(*state, action_sequence, search_states, config.use_search_states, depth, config.max_simulations, true, config.use_forced_playouts);

    open_spiel::ActionsAndProbs search_policy;
    search_policy.reserve(root->children.size());
    std::unordered_map<int, int> search_visits;
    std::unordered_map<int, double> search_priors;
    std::unordered_map<int, double> search_action_values;
    std::unordered_map<int, int> search_model_update_states;
    std::unordered_map<int, int> search_model_updates;
    std::unordered_map<int, int> search_terminal_states;
    std::unordered_map<int, int> search_nonterminal_states;
    for (const SearchNode& c : root->children) {
      search_policy.emplace_back(c.action, std::pow(c.explore_count, 1.0 / config.temperature));

      if (config.write_search_stats) {
        // Update Search Stats
        search_visits[(int) c.action] = c.explore_count;
        search_priors[(int) c.action] = c.prior;
        search_action_values[(int) c.action] = c.total_reward / c.explore_count;
        search_model_update_states[(int) c.action] = 0;
        search_model_updates[(int) c.action] = 0;
        search_terminal_states[(int) c.action] = c.terminal_states;
        search_nonterminal_states[(int) c.action] = c.nonterminal_states;

        for (std::string subtree_state_str : c.subtree_states) {
          int state_updates = update_states.GetUpdateCount(subtree_state_str);
          if (state_updates > 0) {
            search_model_update_states[(int) c.action] += 1;
            search_model_updates[(int) c.action] += state_updates;
          }
        }
      }
    }

    NormalizePolicy(&search_policy);

    double root_value = root->total_reward / root->explore_count;
    trajectory.states.push_back(Trajectory::State{
      state->ObservationTensor(), player, state->LegalActions(), action, std::move(search_policy), root_value, state_str, action_sequence, search_states, depth, true, (*bots)[player]->IsMCTSSolver(), search_visits, search_priors, search_action_values, search_model_update_states, search_model_updates, search_terminal_states, search_nonterminal_states});
    trajectory.returns.resize(2);
    trajectory.returns[player] = root_value;
    trajectory.returns[1 - player] = -root_value;
    trajectory_queue->Push(trajectory, absl::Seconds(10));

    // Sample new random number to see if we continue recursion
    rand_num = dist(*rng);
    if (rand_num < 0.25) {
      return KataGoRecursiveBranch(config, state, action_sequence, depth, vp_eval, bots, rng, kInvalidAction, trajectory_queue, update_states);
    }
    return 0;
  }
}

// KataGo's game branching scheme. Enabled when config.game_branching=true.
// In 5% of games, the game is branched after the first r moves
// Learn more in Appendix D: https://arxiv.org/pdf/1902.10565.pdf
Trajectory KataGoBranch(const AlphaZeroConfig& config, std::shared_ptr<open_spiel::State> state,
                    std::vector<open_spiel::Action> action_sequence, int depth, std::vector<std::unique_ptr<MCTSBot>>* bots,
                    std::mt19937* rng, double cutoff_value, UpdateStates& update_states) {
  Trajectory trajectory;

  while (true) {
    std::string state_str = state->ToString();
    state_str.erase(std::remove(state_str.begin(), state_str.end(), '\n'), state_str.end());

    open_spiel::Player player = state->CurrentPlayer();
    std::unordered_map<std::string, SearchArchiveEntry> search_states;

    // Playout Cap Randomization
    int search_sims = config.max_simulations;
    bool full_search = true;
    if (config.playout_cap) {
      absl::uniform_real_distribution<double> dist(0.0, 1.0);
      double cap_rand_num = dist(*rng);
      if (cap_rand_num > config.full_search_prob) {
        search_sims = config.fast_search;
        full_search = false;
      }
    }

    std::unique_ptr<SearchNode> root = (*bots)[player]->MCTSearch(*state, action_sequence, search_states, config.use_search_states, depth, search_sims, full_search, config.use_forced_playouts);

    open_spiel::ActionsAndProbs policy;
    policy.reserve(root->children.size());
    std::unordered_map<int, int> search_visits;
    std::unordered_map<int, double> search_priors;
    std::unordered_map<int, double> search_action_values;
    std::unordered_map<int, int> search_model_update_states;
    std::unordered_map<int, int> search_model_updates;
    std::unordered_map<int, int> search_terminal_states;
    std::unordered_map<int, int> search_nonterminal_states;
    for (const SearchNode& c : root->children) {
      policy.emplace_back(c.action, std::pow(c.explore_count, 1.0 / config.temperature));

      if (config.write_search_stats) {
        // Update Search Stats
        search_visits[(int) c.action] = c.explore_count;
        search_priors[(int) c.action] = c.prior;
        search_action_values[(int) c.action] = c.total_reward / c.explore_count;
        search_model_update_states[(int) c.action] = 0;
        search_model_updates[(int) c.action] = 0;
        search_terminal_states[(int) c.action] = c.terminal_states;
        search_nonterminal_states[(int) c.action] = c.nonterminal_states;

        for (std::string subtree_state_str : c.subtree_states) {
          int state_updates = update_states.GetUpdateCount(subtree_state_str);
          if (state_updates > 0) {
            search_model_update_states[(int) c.action] += 1;
            search_model_updates[(int) c.action] += state_updates;
          }
        }
      }
    }
    NormalizePolicy(&policy);

    open_spiel::Action action;
    if (depth >= config.temperature_drop) {
      action = root->BestChild().action;
    } else {
      action = open_spiel::SampleAction(policy, *rng).first;
    }

    double root_value = root->total_reward / root->explore_count;

    // Only train on states where full searches were performed
    if (full_search){
      trajectory.states.push_back(Trajectory::State{
        state->ObservationTensor(), player, state->LegalActions(), action, std::move(policy), root_value, state_str, action_sequence, search_states, depth, full_search, (*bots)[player]->IsMCTSSolver(), search_visits, search_priors, search_action_values, search_model_update_states, search_model_updates, search_terminal_states, search_nonterminal_states});
    }

    state->ApplyAction(action);
    action_sequence.push_back(action);
    depth += 1;

    if (state->IsTerminal()) {
      trajectory.returns = state->Returns();
      break;
    } else if (std::abs(root_value) > cutoff_value) {
      trajectory.returns.resize(2);
      trajectory.returns[player] = root_value;
      trajectory.returns[1 - player] = -root_value;
      break;
    }
  }

  return trajectory;
}

// Plays a self-play game or an evaluation match against MCTS-Solver
// Performs KataGo's trajectory initialization if config.katago_init=true
// Performs Go-Exploit's trajectory initialization scheme if config.start_state_prob > 0.0
// Performs KataGo's Playout Cap Randomization if config.playout_cap=true
// Performs KataGo's Forced Playouts + Policy Target Pruning if config.use_forced_playouts=true
Trajectory PlayGame(Logger* logger, int game_num, const AlphaZeroConfig& config, const open_spiel::Game& game,
                    std::vector<std::unique_ptr<MCTSBot>>* bots, std::shared_ptr<VPNetEvaluator> vp_eval,
                    std::mt19937* rng, double temperature, int temperature_drop, double cutoff_value,
                    std::string caller, bool use_search_states, bool use_forced_playouts, Archive& archive,
                    UpdateStates& update_states, bool verbose = false) {
  
  std::shared_ptr<open_spiel::State> state = game.NewInitialState();
  std::vector<std::string> history;
  Trajectory trajectory;

  std::vector<open_spiel::Action> action_sequence;
  int depth = 0;

  // Sample random number to determine whether Go-Exploit begins self-play trajectory from sampled state of interest
  absl::uniform_real_distribution<double> dist(0.0, 1.0);
  double go_exploit_rand_num = dist(*rng);

  // KataGo Trajectory Initialization
  if (config.katago_init && caller == "Training_Actor") {
    // Mean of Exponential Distribution = 1/lambda
    double mean = 0.04 * config.board_width * config.board_length;
    double lambda = 1.0 / mean;

    std::exponential_distribution<double> distribution(lambda);
    double exp_num = distribution(*rng);
    int num_moves = (int) std::floor(exp_num);

    KataGoInitData katago_init_data = KataGoInitializeGame(game, num_moves, vp_eval, rng);
    state = katago_init_data.state;
    depth = katago_init_data.depth;
    action_sequence = katago_init_data.action_sequence;
  }
  // Go-Exploit Trajectory Initialization
  else if (caller == "Training_Actor" && go_exploit_rand_num >= config.start_state_prob) {
    ArchiveEntry entry = archive.Sample();
    // Take Archive State's Action Sequence To Load Archive State
    for (open_spiel::Action seq_action : entry.action_sequence) {
      state->ApplyAction(seq_action);
    }

    depth = entry.depth;
    action_sequence = entry.action_sequence;
  }

  // Looping until a terminal state is reached
  while (true) {
    std::string state_str = state->ToString();
    state_str.erase(std::remove(state_str.begin(), state_str.end(), '\n'), state_str.end());

    open_spiel::Player player = state->CurrentPlayer();
    // Initializing empty hashmap to be populated with search states during MCTS
    std::unordered_map<std::string, SearchArchiveEntry> search_states;

    int search_sims;
    bool full_search;
    if (caller == "Evaluator") {
      search_sims = (*bots)[player]->GetMaxSimulations();
      full_search = false;
    }
    else if (config.playout_cap && caller == "Training_Actor") {
      // Playout Cap Randomization
      double cap_rand_num = dist(*rng);
      if (cap_rand_num > config.full_search_prob) {
        search_sims = config.fast_search;
        full_search = false;
      }
      else {
        search_sims = config.max_simulations;
        full_search = true;
      }
    }
    else {
      search_sims = config.max_simulations;
      full_search = true;
    }

    // MCTS
    std::unique_ptr<SearchNode> root = (*bots)[player]->MCTSearch(*state, action_sequence, search_states, use_search_states, depth, search_sims, full_search, use_forced_playouts);

    open_spiel::ActionsAndProbs policy;
    policy.reserve(root->children.size());
    std::unordered_map<int, int> search_visits;
    std::unordered_map<int, double> search_priors;
    std::unordered_map<int, double> search_action_values;
    std::unordered_map<int, int> search_model_update_states;
    std::unordered_map<int, int> search_model_updates;
    std::unordered_map<int, int> search_terminal_states;
    std::unordered_map<int, int> search_nonterminal_states;
    for (const SearchNode& c : root->children) {
      policy.emplace_back(c.action, std::pow(c.explore_count, 1.0 / temperature));

      if (config.write_search_stats) {
        // Update Search Stats
        search_visits[(int) c.action] = c.explore_count;
        search_priors[(int) c.action] = c.prior;
        search_action_values[(int) c.action] = c.total_reward / c.explore_count;
        search_model_update_states[(int) c.action] = 0;
        search_model_updates[(int) c.action] = 0;
        search_terminal_states[(int) c.action] = c.terminal_states;
        search_nonterminal_states[(int) c.action] = c.nonterminal_states;

        for (std::string subtree_state_str : c.subtree_states) {
          int state_updates = update_states.GetUpdateCount(subtree_state_str);
          if (state_updates > 0) {
            search_model_update_states[(int) c.action] += 1;
            search_model_updates[(int) c.action] += state_updates;
          }
        }
      }
    }

    NormalizePolicy(&policy);

    // Sample action or play action most visited during search
    open_spiel::Action action;
    if (history.size() >= temperature_drop) {
      action = root->BestChild().action;
    } else {
      action = open_spiel::SampleAction(policy, *rng).first;
    }

    double root_value = root->total_reward / root->explore_count;
    trajectory.states.push_back(Trajectory::State{
        state->ObservationTensor(), player, state->LegalActions(), action, std::move(policy), root_value, state_str, action_sequence, search_states, depth, full_search, (*bots)[player]->IsMCTSSolver(), search_visits, search_priors, search_action_values, search_model_update_states, search_model_updates, search_terminal_states, search_nonterminal_states});
    std::string action_str = state->ActionToString(player, action);
    history.push_back(action_str);
    state->ApplyAction(action);
    action_sequence.push_back(action);
    depth += 1;

    if (verbose) {
      logger->Print("Player: %d, action: %s", player, action_str);
    }
    if (state->IsTerminal()) {
      trajectory.returns = state->Returns();
      break;
    } else if (std::abs(root_value) > cutoff_value) {
      trajectory.returns.resize(2);
      trajectory.returns[player] = root_value;
      trajectory.returns[1 - player] = -root_value;
      break;
    }
  }

  logger->Print("Game %d: Returns: %s; Actions: %s", game_num,
                absl::StrJoin(trajectory.returns, " "),
                absl::StrJoin(history, " "));
  return trajectory;
}

// Creates an instantiation of AlphaZero's variant of MCTS
std::unique_ptr<MCTSBot> InitAZBot(const AlphaZeroConfig& config,
                                   const open_spiel::Game& game,
                                   std::shared_ptr<Evaluator> evaluator,
                                   bool evaluation, double uct_c,
                                   int seed) {
  return std::make_unique<MCTSBot>(
      game, std::move(evaluator), uct_c, config.max_simulations,
      /*max_memory_mb=*/10,
      /*solve=*/false,
      /*seed=*/seed,
      /*verbose=*/false, ChildSelectionPolicy::PUCT,
      evaluation ? 0 : config.policy_alpha,
      evaluation ? 0 : config.policy_epsilon);
}

// A training actor thread that generates self-play trajectories for the policy-value network to train upon
// In Go-Exploit Visited States, states visited by these threads are added to the archive in learner()
// If KataGo's branching schemes are enabled, they are called after a self-play trajectory is completed.
void training_actor(const open_spiel::Game& game, const AlphaZeroConfig& config, int num,
           ThreadedQueue<Trajectory>* trajectory_queue,
           std::shared_ptr<VPNetEvaluator> vp_eval, StopToken* stop,
           Archive& archive, UpdateStates& update_states) {
  std::unique_ptr<Logger> logger;
  if (num < 20) {  // Limit the number of open files.
    logger.reset(new FileLogger(config.path, absl::StrCat("actor-", num)));
  } else {
    logger.reset(new NoopLogger());
  }
  unsigned seed1 = std::chrono::system_clock::now().time_since_epoch().count();
  std::mt19937 rng(seed1);
  absl::uniform_real_distribution<double> dist(0.0, 1.0);
  std::vector<std::unique_ptr<MCTSBot>> bots;
  bots.reserve(2);
  for (int player = 0; player < 2; player++) {
    int bot_seed = (int) std::chrono::system_clock::now().time_since_epoch().count();
    bots.push_back(InitAZBot(config, game, vp_eval, false, config.uct_c, bot_seed));
  }
  // Generate self-play trajectories
  for (int game_num = 1; !stop->StopRequested(); ++game_num) {
    double cutoff =
        (dist(rng) < config.cutoff_probability ? config.cutoff_value
                                               : game.MaxUtility() + 1);

    // Play Regular Training Trajectory
    Trajectory original_trajectory = PlayGame(logger.get(), game_num, config, game, &bots, vp_eval, &rng, config.temperature, config.temperature_drop, cutoff, "Training_Actor", false, config.use_forced_playouts, archive, update_states);

    // KataGo Early Game Branching
    if (config.game_branching) {
      double rand_num = dist(rng);

      // Branch from early game position in 5% of games
      if (rand_num < 0.05) {
        int num_moves = -1;

        // Determine which position to branch from
        while (num_moves < 0 || num_moves >= original_trajectory.states.size()) {
          double mean = 0.025 * config.board_width * config.board_length;
          double lambda = 1.0 / mean;

          std::exponential_distribution<double> distribution(lambda);
          num_moves = (int) std::floor(distribution(rng));
        }

        std::vector<open_spiel::Action> branched_action_sequence = original_trajectory.states[num_moves].action_sequence;

        // Restore branched state
        std::shared_ptr<open_spiel::State> state_clone = game.NewInitialState();
        for (open_spiel::Action seq_action : branched_action_sequence) {
          state_clone->ApplyAction(seq_action);
        }

        int branched_depth = original_trajectory.states[num_moves].depth;

        // Sample 3-10 legal moves uniformly at random and evaluate
        std::uniform_int_distribution<int> uni(3, 10);
        int num_actions = (int) uni(rng);

        std::vector<open_spiel::Action> legal_actions = state_clone->LegalActions();
        std::vector<open_spiel::Action> sampled_actions;
        std::sample(legal_actions.begin(), legal_actions.end(), std::back_inserter(sampled_actions), num_actions, rng);

        // Determine which sampled action has the greatest value
        open_spiel::Action best_action = -1;
        double best_value = -std::numeric_limits<double>::infinity();
        std::shared_ptr<open_spiel::State> best_branched_state;
        for (open_spiel::Action curr_action : sampled_actions) {
          std::shared_ptr<open_spiel::State> branched_state = std::move(state_clone->Clone());
          branched_state->ApplyAction(curr_action);
          if (!branched_state->IsTerminal()) {
            open_spiel::Player player = branched_state->CurrentPlayer();
            double state_value = vp_eval->Evaluate(*branched_state)[player];

            if (state_value > best_value) {
              best_value = state_value;
              best_action = curr_action;
              best_branched_state = branched_state;
            }
          }
        }

        if (best_action != -1) {
          branched_action_sequence.push_back(best_action);

          Trajectory branch_trajectory = KataGoBranch(config, best_branched_state, branched_action_sequence, branched_depth + 1, &bots, &rng, cutoff, update_states);
          // Returned trajectory only contains states where full searches were performed
          if (branch_trajectory.states.size() > 0) {
            trajectory_queue->Push(branch_trajectory, absl::Seconds(10));
          }
        }
      }
    }

    // KataGo Position Branching
    if (config.position_branching) {
      // In 2.5% of positions, the game is recursively branched
      for (Trajectory::State& state : original_trajectory.states) {
        double rand_num = dist(rng);

        if (rand_num < 0.025) {
          // Restore branched state
          std::shared_ptr<open_spiel::State> recursive_state_clone = game.NewInitialState();
          for (open_spiel::Action seq_action : state.action_sequence) {
            recursive_state_clone->ApplyAction(seq_action);
          }

          KataGoRecursiveBranch(config, recursive_state_clone, state.action_sequence, state.depth, vp_eval, &bots, &rng, state.action, trajectory_queue, update_states);
        }
      }
    }

    // Create trajectory containing only states where full searches were performed.
    // Policy-value network only trains on states where full searches were performed.
    Trajectory full_search_trajectory;
    for (Trajectory::State& state : original_trajectory.states) {
      if (state.full_search) {
        full_search_trajectory.states.push_back(Trajectory::State{
          state.observation, state.current_player, state.legal_actions, state.action, std::move(state.policy), state.value, state.state_str, state.action_sequence, std::move(state.search_states), state.depth, state.full_search, state.is_mcts_solver, std::move(state.search_visits), std::move(state.search_priors), std::move(state.search_action_values), std::move(state.search_model_update_states), std::move(state.search_model_updates), std::move(state.search_terminal_states), std::move(state.search_nonterminal_states)});
      }
    }
    full_search_trajectory.returns = original_trajectory.returns;

    if (full_search_trajectory.states.size() > 0) {
      trajectory_queue->Push(full_search_trajectory, absl::Seconds(10));
    }
  }
  logger->Print("Got a quit.");
}

// An archive actor thread that produces self-play trajectories always beginning from the initial state of the game.
// Adds search states to the archive in Go-Exploit Search States
void archive_actor(const open_spiel::Game& game, const AlphaZeroConfig& config, int num,
           std::shared_ptr<VPNetEvaluator> vp_eval, StopToken* stop,
           Archive& archive, UpdateStates& update_states) {
  FileLogger logger(config.path, absl::StrCat("archive_actor-", num));
  unsigned seed1 = std::chrono::system_clock::now().time_since_epoch().count();
  std::mt19937 rng(seed1);
  absl::uniform_real_distribution<double> dist(0.0, 1.0);
  std::vector<std::unique_ptr<MCTSBot>> bots;
  bots.reserve(2);
  for (int player = 0; player < 2; player++) {
    int bot_seed = (int) std::chrono::system_clock::now().time_since_epoch().count();
    bots.push_back(InitAZBot(config, game, vp_eval, false, config.uct_c, bot_seed));
  }
  // Produce self-play trajectories
  for (int game_num = 1; !stop->StopRequested(); ++game_num) {
    double cutoff =
        (dist(rng) < config.cutoff_probability ? config.cutoff_value
                                               : game.MaxUtility() + 1);
    
    // Since caller="Archive_Actor", the self-play trajectories always begin from the initial state of the game
    Trajectory trajectory = PlayGame(&logger, game_num, config, game, &bots, vp_eval, &rng, config.temperature, config.temperature_drop, cutoff, "Archive_Actor", config.use_search_states, config.use_forced_playouts, archive, update_states);

    // If Go-Exploit Search States, add search states to the archive
    if (config.use_search_states) {
      std::vector<ArchiveEntry> trajectory_archive_entries;
      for (Trajectory::State& state : trajectory.states) {
        for( const auto& entry : state.search_states ) {
          std::string search_state_str = entry.first;
          SearchArchiveEntry search_archive_entry = entry.second;

          ArchiveEntry search_state_entry = {search_archive_entry.action_sequence, search_archive_entry.depth};
          trajectory_archive_entries.push_back(search_state_entry);
        }
      }
      // Update Archive. Transfer archive entries from step_archive to archive
      archive.Update(trajectory_archive_entries);
    }
  }
}

// Processes the evaluation match results against MCTS-Solver
class EvalResults {
 public:
  explicit EvalResults(int count, int evaluation_window) {
    evaluation_window_ = evaluation_window;
    step_ = 1;
    matches_played_.reserve(count);
    results_sum_.reserve(count);
    results_.reserve(count);
    for (int i = 0; i < count; ++i) {
      results_.emplace_back(evaluation_window);
      matches_played_.emplace_back(0);
      results_sum_.emplace_back(0);
    }
  }

  // How many evals per difficulty.
  int EvalCount() {
    absl::MutexLock lock(&m_);
    return eval_num_ / results_.size();
  }

  // Which eval to do next: difficulty, player0.
  std::pair<int, bool> Next() {
    absl::MutexLock lock(&m_);
    int next = eval_num_ % (results_.size() * 2);
    eval_num_ += 1;
    return {next / 2, next % 2};
  }

  void Add(int i, double value, double game_time, double game_length) {
    absl::MutexLock lock(&m_);
    results_[i].Add(value);
    if (step_ > matches_played_[i].size()) {
      for (int j = matches_played_[i].size(); j < step_; ++j) {
        matches_played_[i].push_back(0);
        results_sum_[i].push_back(0);
      }
    }
    matches_played_[i][step_ - 1] += 1;
    results_sum_[i][step_ - 1] += value;
  }

  std::vector<double> AvgResults() {
    absl::MutexLock lock(&m_);
    std::vector<double> out;
    out.reserve(results_.size());
    for (const auto& result : results_) {
      out.push_back(result.Empty() ? 0
                                   : (absl::c_accumulate(result.Data(), 0.0) /
                                      result.Size()));
    }
    return out;
  }

  std::vector<std::vector<double>> StepAvgResults(const std::string &path) {
    absl::MutexLock lock(&m_);

    // Ensure size of matches_played_[i] and results_sum_[i] = step_ (If no eval matches finished for a specific difficulty in the given learning step, the size of the vectors won't match step_. Need to address that here)
    for (int i = 0; i < results_.size(); ++i) {
      if (step_ > matches_played_[i].size()) {
        for (int j = matches_played_[i].size(); j < step_; ++j) {
          matches_played_[i].push_back(0);
          results_sum_[i].push_back(0);
        }
      }
    }

    // Output Vectors
    std::vector<std::vector<double>> out;

    std::vector<double> step_weighted_performance;
    step_weighted_performance.reserve(results_.size());

    std::vector<double> match_weighted_performance;
    match_weighted_performance.reserve(results_.size());

    // Compute Avg Performance Over Previous "Evaluation Window" Learning Steps
    int min_step = std::max((int) (step_ - evaluation_window_), (int) 0);

    for (int i = 0; i < results_.size(); ++i){
      if (matches_played_[i].size() > 0) {
        double steps = 0;
        double avg_outcome_sum = 0;
        double outcome_sum = 0;
        double total_eval_matches = 0;
        for (int j = min_step; j < step_; ++j) {
          if (matches_played_[i][j] > 0) {
            steps += 1;
            total_eval_matches += matches_played_[i][j];
            avg_outcome_sum += (results_sum_[i][j] / matches_played_[i][j]);
            outcome_sum += results_sum_[i][j];
          }
        }
        if (steps > 0) {
          step_weighted_performance.push_back(avg_outcome_sum / steps);
          if (std::abs(avg_outcome_sum / steps) > 1.0) {
            std::cout << "Step Weighted Value Greater Than 1.0" << std::endl;
            std::cout << "avg_outcome_sum: " << std::to_string(avg_outcome_sum) << std::endl;
            std::cout << "steps: " << std::to_string(steps) << std::endl;
          }
        }
        else {
          step_weighted_performance.push_back(-1.0);
        }
        if (total_eval_matches > 0) {
          match_weighted_performance.push_back(outcome_sum / total_eval_matches);
          if (std::abs(outcome_sum / total_eval_matches) > 1.0) {
            std::cout << "Match Weighted Value Greater Than 1.0" << std::endl;
            std::cout << "outcome_sum: " << std::to_string(outcome_sum) << std::endl;
            std::cout << "total_eval_matches: " << std::to_string(total_eval_matches) << std::endl;
          }
        }
        else {
          match_weighted_performance.push_back(-1.0);
        }
      }
      else {
        std::cout << "Difficulty " << std::to_string(i) << " Step " << std::to_string(step_) << " No Eval Matches Yet" << std::endl;
        step_weighted_performance.push_back(-1.0);
        match_weighted_performance.push_back(-1.0);
      }
    }

    out.push_back(step_weighted_performance);
    out.push_back(match_weighted_performance);

    step_ += 1;

    return out;
  }

  void WriteResults(const std::string &path) {
    absl::MutexLock lock(&m_);
    std::ofstream eval_results_file;
    eval_results_file.open(path + "/Eval_Results.csv", std::ios::out);
    eval_results_file << std::to_string(eval_num_) << std::endl;
    for (int i = 0; i < results_.size(); ++i){
      const std::vector<double>& level_results = results_[i].Data();
      for (double outcome : level_results) {
        eval_results_file << std::to_string(outcome) << ",";
      }
      eval_results_file << std::endl;
      int64_t total_added = results_[i].TotalAdded();
      eval_results_file << std::to_string(total_added) << std::endl;
    }
    eval_results_file.close();
  }

  int GetLearningStep() {
    absl::MutexLock lock(&m_);
    return step_;
  }

 private:
  std::vector<CircularBuffer<double>> results_;
  std::vector<std::vector<double>> matches_played_;
  std::vector<std::vector<double>> results_sum_;
  double evaluation_window_;
  int step_ = 1;
  int eval_num_ = 0;
  absl::Mutex m_;
};

// An evaluator thread that plays evaluation matches against different difficulty levels of MCTS-Solver
void evaluator(const open_spiel::Game& game, const AlphaZeroConfig& config,
               int num, EvalResults* results,
               std::shared_ptr<VPNetEvaluator> vp_eval, StopToken* stop,
               Archive& archive, UpdateStates& update_states) {
  FileLogger logger(config.path, absl::StrCat("evaluator-", num));
  unsigned seed1 = std::chrono::system_clock::now().time_since_epoch().count();
  std::mt19937 rng(seed1);
  int rollout_seed = (int) std::chrono::system_clock::now().time_since_epoch().count();
  auto rand_evaluator = std::make_shared<RandomRolloutEvaluator>(1, rollout_seed);

  // Play evaluation matches against MCTS-Solver
  for (int game_num = 1; !stop->StopRequested(); ++game_num) {
    // Determine "difficulty level" of MCTS-Solver (number of search iterations)
    auto [difficulty, first] = results->Next();
    int learning_step = results->GetLearningStep();
    int az_player = first ? 0 : 1;
    int rand_max_simulations =
        config.max_simulations * std::pow(10, difficulty / 2.0);
    std::vector<std::unique_ptr<MCTSBot>> bots;
    bots.reserve(2);
    int bot1_seed = (int) std::chrono::system_clock::now().time_since_epoch().count();
    bots.push_back(InitAZBot(config, game, vp_eval, true, config.uct_c, bot1_seed));
    int bot2_seed = (int) std::chrono::system_clock::now().time_since_epoch().count();
    bots.push_back(std::make_unique<MCTSBot>(
        game, rand_evaluator, config.eval_uct_c, rand_max_simulations,
        /*max_memory_mb=*/1000,
        /*solve=*/true,
        /*seed=*/bot2_seed,
        /*verbose=*/false, ChildSelectionPolicy::UCT));
    if (az_player == 1) {
      std::swap(bots[0], bots[1]);
    }

    logger.Print("Running MCTS with %d simulations", rand_max_simulations);
    Trajectory trajectory = PlayGame(
        &logger, game_num, config, game, &bots, vp_eval, &rng, /*temperature=*/1,
        /*temperature_drop=*/0, /*cutoff_value=*/game.MaxUtility() + 1, "Evaluator", false, false, archive, update_states);
    
    // Write Eval Search Stats
    if (config.write_search_stats) {
      absl::MutexLock lock(&eval_mutex_);
      for (Trajectory::State& state : trajectory.states) {
        if (!state.is_mcts_solver) {
          std::ofstream eval_search_stats_file;
          eval_search_stats_file.open(config.path + "/Eval_Search_Stats_Step_" + std::to_string(learning_step) + "_Level_" + std::to_string(difficulty) + ".csv", std::ios::out | std::ios::app);
          eval_search_stats_file << state.state_str << "," << std::to_string(state.action) << "," << std::to_string(state.depth) << std::endl;
          for( auto& entry : state.search_visits ) {
            int action = entry.first;
            eval_search_stats_file << std::to_string(action) << "," << std::to_string(state.search_visits[action]) << "," << std::to_string(state.search_priors[action]) << "," << std::to_string(state.search_action_values[action]) << "," << std::to_string(state.search_model_update_states[action]) << "," << std::to_string(state.search_model_updates[action]) << "," << std::to_string(state.search_terminal_states[action]) << "," << std::to_string(state.search_nonterminal_states[action]) << std::endl;
          }
          eval_search_stats_file.close();
        }
      }
    }

    results->Add(difficulty, trajectory.returns[az_player], eval_game_time, (double) trajectory.states.size());
    logger.Print("Game %d: AZ: %5.2f, MCTS: %5.2f, MCTS-sims: %d, length: %d",
                 game_num, trajectory.returns[az_player],
                 trajectory.returns[1 - az_player], rand_max_simulations,
                 trajectory.states.size());
  }
  logger.Print("Got a quit.");
}

// A learner thread that produces training samples from the self-play trajectories generated by the training actors.
// Once a quota of new training tuples have been added to the experience replay buffer, training batches are sampled to update the policy-value network
// When use_search_states=false and start_state_prob < 1.0 (Go-Exploit Visited States), the learner adds the training actors' visited states to the archive
void learner(const open_spiel::Game& game, const AlphaZeroConfig& config,
             DeviceManager* device_manager,
             std::shared_ptr<VPNetEvaluator> eval,
             ThreadedQueue<Trajectory>* trajectory_queue,
             EvalResults* eval_results, StopToken* stop,
             const StartInfo& start_info,
             Archive& archive, UpdateStates& update_states) {
  absl::Time time_start = absl::Now();
  FileLogger logger(config.path, "learner", "a");
  DataLoggerJsonLines data_logger(
      config.path, "learner", true, "a", start_info.start_time);
  unsigned seed = std::chrono::system_clock::now().time_since_epoch().count();
  std::mt19937 rng(seed);

  int device_id = 0;  // Do not change, the first device is the learner.
  logger.Print("Running the learner on device %d: %s", device_id,
               device_manager->Get(0, device_id)->Device());

  SerializableCircularBuffer<VPNetModel::TrainInputs> replay_buffer(
      config.replay_buffer_size);
  if (start_info.start_step > 1) {
    std::cout << "Loading Saved Replay Buffer" << std::endl;
    replay_buffer.LoadBuffer(config.path + "/replay_buffer.data");
    std::cout << "Finished Loading Saved Replay Buffer" << std::endl;
  }
  // The quota of new training samples added to the experience replay buffer each learning step.
  // Also the number of samples used to update the policy-value network each learning step.
  int learn_rate = config.replay_buffer_size / config.replay_buffer_reuse;

  int64_t total_trajectories = start_info.total_trajectories;

  // Holds states of interest visited/observed in the current learning step. Transferred over to real archive at the end of the learning step. Only used by Go-Exploit Visited States (no search states)
  std::vector<ArchiveEntry> step_archive;

  const int stage_count = 7;
  std::vector<open_spiel::BasicStats> value_accuracies(stage_count);
  std::vector<open_spiel::BasicStats> value_predictions(stage_count);
  open_spiel::BasicStats game_lengths;
  open_spiel::HistogramNumbered game_lengths_hist(game.MaxGameLength() + 1);

  open_spiel::HistogramNamed outcomes({"Player1", "Player2", "Draw"});
  // Actor threads have likely been contributing for a while, so put `last` in
  // the past to avoid a giant spike on the first step.
  absl::Time last = absl::Now() - absl::Seconds(60);
  // Iterating through learning steps
  for (int step = start_info.start_step; !stop->StopRequested() && (config.max_steps == 0 || step <= config.max_steps); ++step) {
    absl::Time step_start = absl::Now();
    std::cout << "Step: " << std::to_string(step) << std::endl;

    outcomes.Reset();
    game_lengths.Reset();
    game_lengths_hist.Reset();

    step_archive.clear();

    for (auto& value_accuracy : value_accuracies) {
      value_accuracy.Reset();
    }
    for (auto& value_prediction : value_predictions) {
      value_prediction.Reset();
    }

    // Collect self-play trajectories
    int queue_size = trajectory_queue->Size();
    int num_states = 0;
    int num_trajectories = 0;
    // Learning step ends when learn_rate new training samples have been produced
    while (!stop->StopRequested() && num_states < learn_rate) {
      // Wait until a training actor adds a new self-play trajectory to the trajectory_queue
      absl::optional<Trajectory> trajectory = trajectory_queue->Pop();
      if (trajectory) {
        num_trajectories += 1;
        total_trajectories += 1;
        game_lengths.Add(trajectory->states.size());
        game_lengths_hist.Add(trajectory->states.size());

        double p1_outcome = trajectory->returns[0];
        outcomes.Add(p1_outcome > 0 ? 0 : (p1_outcome < 0 ? 1 : 2));

        // Create training samples for each state in the trajectory
        for (Trajectory::State& state : trajectory->states) {
          replay_buffer.Add(VPNetModel::TrainInputs{state.state_str, state.legal_actions,
                                                    state.observation,
                                                    state.policy, p1_outcome});
          num_states += 1;

          // If Go-Exploit Visited States, update Step Archive
          if (config.start_state_prob < 1.0 && !config.use_search_states) {
            ArchiveEntry state_entry = {state.action_sequence, state.depth};
            step_archive.push_back(state_entry);
          }

          // Write search statistics to a file
          if (config.write_search_stats) {
            /*
            Search Stats File organized as follows:
            - Self-Play State, Action played, Depth
            - Action 0: visits, prior, Q(s,a), states in subtree used to update model, total number of times states in subtree have been used to update model, number of terminal states in subtree, number of nonterminal states in subtree
            ...
            - Action k: visits, prior, Q(s,a), states in subtree used to update model, total number of times states in subtree have been used to update model, number of terminal states in subtree, number of nonterminal states in subtree
            */
            std::ofstream self_play_search_stats_file;
            self_play_search_stats_file.open(config.path + "/Self_Play_Search_Stats_Step_" + std::to_string(step) + ".csv", std::ios::out | std::ios::app);
            self_play_search_stats_file << state.state_str << "," << std::to_string(state.action) << "," << std::to_string(state.depth) << std::endl;
            for( auto& entry : state.search_visits ) {
              int action = entry.first;
              self_play_search_stats_file << std::to_string(action) << "," << std::to_string(state.search_visits[action]) << "," << std::to_string(state.search_priors[action]) << "," << std::to_string(state.search_action_values[action]) << "," << std::to_string(state.search_model_update_states[action]) << "," << std::to_string(state.search_model_updates[action]) << "," << std::to_string(state.search_terminal_states[action]) << "," << std::to_string(state.search_nonterminal_states[action]) << std::endl;
            }
            self_play_search_stats_file.close();
          }
        }

        for (int stage = 0; stage < stage_count; ++stage) {
          // Scale for the length of the game
          int index = (trajectory->states.size() - 1) *
                      static_cast<double>(stage) / (stage_count - 1);
          const Trajectory::State& s = trajectory->states[index];
          value_accuracies[stage].Add(
              (s.value >= 0) == (trajectory->returns[s.current_player] >= 0));
          value_predictions[stage].Add(abs(s.value));
        }
      }
    }

    absl::Time now = absl::Now();
    double seconds = absl::ToDoubleSeconds(now - last);

    logger.Print("Step: %d", step);
    logger.Print(
        "Collected %5d states from %3d games, %.1f states/s; "
        "%.1f states/(s*actor), game length: %.1f",
        num_states, num_trajectories, num_states / seconds,
        num_states / (config.training_actors * seconds),
        static_cast<double>(num_states) / num_trajectories);
    logger.Print("Queue size: %d. Buffer size: %d. States seen: %d", queue_size,
                 replay_buffer.Size(), replay_buffer.TotalAdded());

    if (stop->StopRequested()) {
      break;
    }

    last = now;

    // Save experience replay buffer for checkpointing
    replay_buffer.SaveBuffer(config.path + "/replay_buffer.data");

    // Save Eval Results
    eval_results->WriteResults(config.path);

    absl::Time model_update_start = absl::Now();
    VPNetModel::LossInfo losses;
    std::string checkpoint_path = "";
    {  // Extra scope to return the device for use for inference asap.
      DeviceManager::DeviceLoan learn_model =
          device_manager->Get(config.train_batch_size, device_id);

      // Let the device manager know that the first device is now
      // off-limits for inference and should only be used for learning
      // (if config.explicit_learning == true).
      device_manager->SetLearning(config.explicit_learning);

      // Update policy-value network
      for (int i = 0; i < learn_rate / config.train_batch_size; i++) {
        std::vector<VPNetModel::TrainInputs> learn_inputs = replay_buffer.Sample(&rng, config.train_batch_size);
        if (config.write_search_stats) {
          update_states.Update(learn_inputs);
        }
        losses += learn_model->Learn(learn_inputs);
      }

      if (step > 0 && step % 50 == 0 && config.write_search_stats) {
        update_states.Write(config.path, step);
      }

      // Always save a checkpoint, either for keeping or for loading the weights
      // to the other sessions. It only allows numbers, so use -1 as "latest".
      // std::cout << "Save Updated Model Checkpoint" << std::endl;
      checkpoint_path = learn_model->SaveCheckpoint(VPNetModel::kMostRecentCheckpointStep);
      if (step % config.checkpoint_freq == 0) {
        learn_model->SaveCheckpoint(step);
      }

      // The device manager can now once again use the first device for
      // inference (if it could not before).
      device_manager->SetLearning(false);
      // std::cout << "Explicit Learning Set To False" << std::endl;
    }

    // Putting the updated policy-value network on all devices
    if (device_manager->Count() > 0) {
      for (int i = 0; i < device_manager->Count(); ++i) {
        if (i != device_id) {
          device_manager->SetModelUpdating(i, true);
          device_manager->Get(0, i)->LoadCheckpoint(checkpoint_path);
          device_manager->SetModelUpdating(i, false);
        }
      }
    }

    logger.Print("Checkpoint saved: %s", checkpoint_path);

    absl::Time model_update_end = absl::Now();
    double model_update_time = absl::ToDoubleSeconds(model_update_end - model_update_start);
    std::cout << "Model Update Time: " << std::to_string(model_update_time) << std::endl;

    // If Go-Exploit Visited States, Update Archive. Transfer archive entries from step_archive to archive
    if (config.start_state_prob < 1.0 && !config.use_search_states) {
      archive.Update(step_archive);
    }

    std::vector<std::vector<double>> step_eval_results = eval_results->StepAvgResults(config.path);
    std::vector<double> step_weighted_performance = step_eval_results[0];
    std::vector<double> match_weighted_performance = step_eval_results[1];

    DataLogger::Record record = {
        {"step", step},
        {"total_states", replay_buffer.TotalAdded()},
        {"states_per_s", num_states / seconds},
        {"states_per_s_actor", num_states / (config.training_actors * seconds)},
        {"total_trajectories", total_trajectories},
        {"trajectories_per_s", num_trajectories / seconds},
        {"queue_size", queue_size},
        {"game_length", game_lengths.ToJson()},
        {"game_length_hist", game_lengths_hist.ToJson()},
        {"outcomes", outcomes.ToJson()},
        {"value_accuracy",
         json::TransformToArray(value_accuracies,
                                [](auto v) { return v.ToJson(); })},
        {"value_prediction",
         json::TransformToArray(value_predictions,
                                [](auto v) { return v.ToJson(); })},
        {"eval", json::Object({
                     {"count", eval_results->EvalCount()},
                     {"window_results", json::CastToArray(eval_results->AvgResults())},
                     {"step_weighted_results", json::CastToArray(step_weighted_performance)},
                     {"match_weighted_results", json::CastToArray(match_weighted_performance)},
                 })},
        {"batch_size", eval->BatchSizeStats().ToJson()},
        {"batch_size_hist", eval->BatchSizeHistogram().ToJson()},
        {"loss", json::Object({
                     {"policy", losses.Policy()},
                     {"value", losses.Value()},
                     {"l2reg", losses.L2()},
                     {"sum", losses.Total()},
                 })},
    };
    eval->ResetBatchSizeStats();
    logger.Print("Losses: policy: %.4f, value: %.4f, l2: %.4f, sum: %.4f",
                 losses.Policy(), losses.Value(), losses.L2(), losses.Total());

    LRUCacheInfo cache_info = eval->CacheInfo();
    if (cache_info.size > 0) {
      logger.Print(absl::StrFormat(
          "Cache size: %d/%d: %.1f%%, hits: %d, misses: %d, hit rate: %.3f%%",
          cache_info.size, cache_info.max_size, 100.0 * cache_info.Usage(),
          cache_info.hits, cache_info.misses, 100.0 * cache_info.HitRate()));
      eval->ClearCache();
    }
    record.emplace("cache",
                   json::Object({
                       {"size", cache_info.size},
                       {"max_size", cache_info.max_size},
                       {"usage", cache_info.Usage()},
                       {"requests", cache_info.Total()},
                       {"requests_per_s", cache_info.Total() / seconds},
                       {"hits", cache_info.hits},
                       {"misses", cache_info.misses},
                       {"misses_per_s", cache_info.misses / seconds},
                       {"hit_rate", cache_info.HitRate()},
                   }));

    data_logger.Write(record);
    logger.Print("");

    absl::Time step_end = absl::Now();
    double step_time = absl::ToDoubleSeconds(step_end - step_start);
    double elapsed_time = absl::ToDoubleSeconds(step_end - time_start);
    std::cout << "Step Time: " << std::to_string(step_time) << std::endl;
    std::cout << "Elapsed Time: " << std::to_string(elapsed_time) << std::endl;
  }
}

// Runs AlphaZero/Go-Exploit for config.max_steps learning steps
bool AlphaZero(AlphaZeroConfig config, StopToken* stop, bool resuming) {
  // Loads config.game as the environment. If config.game=go, sets the board size to 9x9
  std::shared_ptr<const open_spiel::Game> game;
  if (config.game == "go") {
    int board_size = 9;
    game = LoadGame("go", {{"board_size", open_spiel::GameParameter(board_size)}});
  }
  else {
    game = open_spiel::LoadGame(config.game);
  }

  open_spiel::GameType game_type = game->GetType();
  if (game->NumPlayers() != 2)
    open_spiel::SpielFatalError("AlphaZero can only handle 2-player games.");
  if (game_type.reward_model != open_spiel::GameType::RewardModel::kTerminal)
    open_spiel::SpielFatalError("Game must have terminal rewards.");
  if (game_type.dynamics != open_spiel::GameType::Dynamics::kSequential)
    open_spiel::SpielFatalError("Game must have sequential turns.");
  if (game_type.chance_mode != open_spiel::GameType::ChanceMode::kDeterministic)
    open_spiel::SpielFatalError("Game must be deterministic.");

  file::Mkdirs(config.path);
  if (!file::IsDirectory(config.path)) {
    std::cerr << config.path << " is not a directory." << std::endl;
    return false;
  }

  std::cout << "Logging directory: " << config.path << std::endl;

  if (config.graph_def.empty()) {
    config.graph_def = "vpnet.pb";
    std::string model_path = absl::StrCat(config.path, "/", config.graph_def);
    if (file::Exists(model_path)) {
      std::cout << "Overwriting existing model: " << model_path << std::endl;
    } else {
      std::cout << "Creating model: " << model_path << std::endl;
    }
    // Creates file with policy-value network's hyperparameter values
    SPIEL_CHECK_TRUE(CreateGraphDef(
        *game, config.learning_rate, config.weight_decay, config.path,
        config.graph_def, config.nn_model, config.nn_width, config.nn_depth));
  } else {
    std::string model_path = absl::StrCat(config.path, "/", config.graph_def);
    if (file::Exists(model_path)) {
      std::cout << "Using existing model: " << model_path << std::endl;
    } else {
      std::cout << "Model not found: " << model_path << std::endl;
    }
  }

  std::cout << "Playing game: " << config.game << std::endl;

  config.inference_batch_size = std::max(
      1,
      std::min(config.inference_batch_size, config.training_actors + config.archive_actors + config.evaluators));

  config.inference_threads =
      std::max(1, std::min(config.inference_threads,
                           (1 + config.training_actors + config.archive_actors + config.evaluators) / 2));

  {
    file::File fd(config.path + "/config.json", "w");
    fd.Write(json::ToString(config.ToJson(), true) + "\n");
  }

  StartInfo start_info = {/*start_time=*/absl::Now(),
                          /*start_step=*/1,
                          /*model_checkpoint_step=*/0,
                          /*total_trajectories=*/0};
  if (resuming) {
    std::cout << "Load Start Info" << std::endl;
    start_info = StartInfoFromLearnerJson(config.path);
    std::cout << "Loaded Start Step: " << std::to_string(start_info.start_step) << std::endl;
    std::cout << "Loaded Model Checkpoint: " << std::to_string(start_info.model_checkpoint_step) << std::endl;
  }

  // DeviceManager is responsible for allocating inference requests to gpus
  DeviceManager device_manager;
  for (const absl::string_view& device : absl::StrSplit(config.devices, ',')) {
    device_manager.AddDevice(
        VPNetModel(*game, config.path, config.graph_def, std::string(device)));
  }

  if (device_manager.Count() == 0) {
    std::cerr << "No devices specified?" << std::endl;
    return false;
  }

  // The explicit_learning option should only be used when multiple
  // devices are available (so that inference can continue while
  // also undergoing learning).
  if (device_manager.Count() <= 1 && config.explicit_learning) {
    std::cerr << "Explicit learning can only be used with multiple devices."
              << std::endl;
    return false;
  }

  std::cerr << "Loading model from step " << start_info.model_checkpoint_step
            << std::endl;
  {  // Make sure they're all in sync.
    if (!resuming) {
      std::cout << "Save Randomly Initialized Policy-Value Network" << std::endl;
      device_manager.Get(0)->SaveCheckpoint(start_info.model_checkpoint_step);
    }
    // Putting the current policy-value network on all devices
    for (int i = 0; i < device_manager.Count(); ++i) {
      device_manager.Get(0, i)->LoadCheckpoint(
          start_info.model_checkpoint_step);
    }
  }

  auto eval = std::make_shared<VPNetEvaluator>(
      &device_manager, config.inference_batch_size, config.inference_threads,
      config.inference_cache, (config.training_actors + config.archive_actors + config.evaluators) / 16);

  ThreadedQueue<Trajectory> trajectory_queue(config.replay_buffer_size /
                                             config.replay_buffer_reuse);

  EvalResults eval_results(config.eval_levels, config.evaluation_window);

  // Initialize Archive
  unsigned seed = std::chrono::system_clock::now().time_since_epoch().count();
  Archive archive(config.archive_type, config.max_archive_size, seed);

  // States used to update the policy-value network
  UpdateStates update_states;

  // Initialize training actor threads, archive actor threads, and learner thread
  std::vector<Thread> training_actors;
  training_actors.reserve(config.training_actors);
  for (int i = 0; i < config.training_actors; ++i) {
    training_actors.emplace_back(
        [&, i]() { training_actor(*game, config, i, &trajectory_queue, eval, stop, archive, update_states); });
  }
  std::vector<Thread> archive_actors;
  archive_actors.reserve(config.archive_actors);
  for (int i = 0; i < config.archive_actors; ++i) {
    archive_actors.emplace_back(
        [&, i]() { archive_actor(*game, config, i, eval, stop, archive, update_states); });
  }
  std::vector<Thread> evaluators;
  evaluators.reserve(config.evaluators);
  for (int i = 0; i < config.evaluators; ++i) {
    evaluators.emplace_back(
        [&, i]() { evaluator(*game, config, i, &eval_results, eval, stop, archive, update_states); });
  }
  learner(*game, config, &device_manager, eval, &trajectory_queue,
          &eval_results, stop, start_info, archive, update_states);

  if (!stop->StopRequested()) {
    stop->Stop();
  }

  // Empty the queue so that the actors can exit.
  trajectory_queue.BlockNewValues();
  trajectory_queue.Clear();

  std::cout << "Joining all the threads." << std::endl;
  for (auto& t : training_actors) {
    t.join();
  }
  for (auto& t : archive_actors) {
    t.join();
  }
  for (auto& t : evaluators) {
    t.join();
  }
  std::cout << "Exiting cleanly." << std::endl;
  return true;
}

}  // namespace torch_az
}  // namespace algorithms
}  // namespace open_spiel
